{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b47efc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=true\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
      "Cuda compilation tools, release 11.1, V11.1.105\n",
      "Build cuda_11.1.TC455_06.29190527_0\n",
      "Thu Nov 10 08:20:25 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA TITAN X ...  Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 37%   60C    P5    24W / 250W |      2MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN X ...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 31%   49C    P8    10W / 250W |      2MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA TITAN X ...  Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 30%   47C    P8    10W / 250W |      2MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA TITAN X ...  Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 26%   43C    P8    10W / 250W |      2MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://192.168.45.100:3128\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://192.168.45.100:3128\"\n",
    "!pip install monai neurokit2 wfdb monai pytorch_lightning==1.7.7 wandb libauc==1.2.0 --upgrade --quiet\n",
    "\n",
    "gpus= \"0,1,2,3\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus\n",
    "os.environ[\"WANDB_API_KEY\"] = '6cd6a2f58c8f4625faaea5c73fe110edab2be208'\n",
    "%env WANDB_SILENT=true\n",
    "\n",
    "!nvcc -V\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a70c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_defaults = dict(\n",
    "    dataSeed = 2,\n",
    "    srTarget = 250,\n",
    "    featureLength = 1024,\n",
    "    sampler = False, #True,\n",
    "    inChannels = 1,\n",
    "    outChannels = 2,\n",
    "    modelName='efficientnet-b0',\n",
    "    norm = 'instance',\n",
    "    upsample = 'deconv', #'pixelshuffle', # 'nontrainable'\n",
    "    supervision = \"NONE\",\n",
    "    skipModule = \"NONE\",\n",
    "    trainaug = 'NONE',\n",
    "    \n",
    "    project = 'PVC_NET',\n",
    "    path_logRoot = '20221110_final',\n",
    "    spatial_dims = 1,\n",
    "    learning_rate = 4e-3,\n",
    "    batch_size = 256,# 256\n",
    "    dropout=0,\n",
    "    thresholdRPeak= 0.5,\n",
    "    skipASPP = \"NONE\",\n",
    "    lossFn='focalloss',\n",
    "    se = 'se',\n",
    ")\n",
    "\n",
    "sweep_config = {\n",
    "  \"project\" : config_defaults['project'],\n",
    "  \"name\" : config_defaults[\"path_logRoot\"], # sweep run name!!\n",
    "  # \"method\" : \"bayes\",\n",
    "  \"method\" : \"grid\",\n",
    "  \"metric\": {\n",
    "      # \"name\":\"val_loss\",\n",
    "      # \"goal\":\"minimize\"},      \n",
    "      \"name\":\"testMIT_AUPRC_Class1Raw\",\n",
    "      \"goal\":\"maximize\",\n",
    "  },      \n",
    "  \"parameters\" : \n",
    "    {\n",
    "    \"srTarget\":{\"values\": [125, 250, 360]}, # True, False\n",
    "    \"featureLength\":{\"values\": [512, 1024]}, # True, False\n",
    "    \"dataSeed\":{\"values\": [2]}, # True, False\n",
    "    \"sampler\":{\"values\": [True]}, # True, False\n",
    "    \"lossFn\":{\"values\": ['bceloss']}, # ,'bceloss'\n",
    "    \"modelName\":{\"values\": ['efficientnet-b0']}, # 'resnet18', 'resnet34', 'resnet50', 'efficientnet-b0','efficientnet-b1','efficientnet-b2','efficientnet-b3','efficientnet-b4'\n",
    "    \"norm\":{\"values\": ['instance','batch']}, # 'instance','batch'\n",
    "    \"se\":{\"values\": ['se']}, # 'se','acm','nlnn','deeprft','cbam'\n",
    "    \"dropout\":{\"values\": [0.1]}, \n",
    "    \"upsample\":{\"values\": ['pixelshuffle']}, # ['pixelshuffle','deconv','nontrainable']\n",
    "    \"supervision\":{\"values\": ['TYPE2']}, # 'NONE','TYPE1','TYPE2'\n",
    "    \"outChannels\":{\"values\": [2]}, # 2,3,4\n",
    "    \"trainaug\":{'values':['NEUROKIT']}, #\"NONE\",'NEUROKIT','NEUROKIT2',\"AUDIOMENTATION\"\n",
    "    \"skipModule\":{\"values\": ['NONE','ACM2_0_BOTTOM5','ACM4_0_BOTTOM5','ACM8_0_BOTTOM5','SE_BOTTOM5','NLNN_BOTTOM5','CBAM_BOTTOM5','FFC_BOTTOM5','DEEPRFT_BOTTOM5']}, # ,'SE_BOTTOM5','NLNN_BOTTOM5','CBAM_BOTTOM5','FFC_BOTTOM5','DEEPRFT_BOTTOM5','ACM2_BOTTOM5','ACM4_BOTTOM5','ACM8_BOTTOM5'\n",
    "    \"skipASPP\":{\"values\": ['NONE']}, # 'NONE','BOTTOM5'\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d527581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-10 08:20:29,691 - Bagua cannot detect bundled NCCL library, Bagua will try to use system NCCL instead. If you encounter any error, please run `import bagua_core; bagua_core.install_deps()` or the `bagua_install_deps.py` script to install bundled libraries.\n",
      "2022-11-10 08:20:30,088 - Global seed set to 42\n",
      "Number of workers: 12\n",
      "multiprocessing.cpu_count() 12\n",
      "cuda.is_available True\n",
      "cuda\n",
      "MONAI version: 1.0.1\n",
      "Numpy version: 1.22.3\n",
      "Pytorch version: 1.10.2+cu111\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 8271a193229fe4437026185e218d5b06f7c8ce69\n",
      "MONAI __file__: /home/kevin/.local/lib/python3.9/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 3.2.2\n",
      "scikit-image version: 0.19.2\n",
      "Pillow version: 9.1.1\n",
      "Tensorboard version: 2.10.1\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.11.3+cu111\n",
      "tqdm version: 4.63.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.1\n",
      "pandas version: 1.4.2\n",
      "einops version: 0.4.1\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 1.26.1\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os, sys, shutil\n",
    "import multiprocessing\n",
    "import random\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from natsort import natsorted\n",
    "\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "from scipy.signal import butter, filtfilt, lfilter\n",
    "from scipy.signal import kaiserord, firwin, filtfilt, butter\n",
    "from scipy.ndimage import label, binary_closing\n",
    "from skimage import morphology\n",
    "from scipy import ndimage\n",
    "\n",
    "import kornia\n",
    "import neurokit2 as nk\n",
    "import librosa as lb\n",
    "import soundfile as sf\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve, classification_report, auc\n",
    "\n",
    "import cv2\n",
    "import monai\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.config import print_config\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import *\n",
    "from pytorch_lightning.loggers import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import *\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    monai.utils.misc.set_determinism(seed=seed)\n",
    "    pl.seed_everything(seed,True)    \n",
    "    \n",
    "def get_device() -> torch.device:\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "set_seed()\n",
    "device = get_device()\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(\"Number of workers:\", NUM_WORKERS)\n",
    "print('multiprocessing.cpu_count()', multiprocessing.cpu_count())\n",
    "print('cuda.is_available', torch.cuda.is_available())\n",
    "print(device)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "286e6708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Youden_index(y_true, y_score):\n",
    "    '''Find data-driven cut-off for classification    \n",
    "    Cut-off is determied using Youden's index defined as sensitivity + specificity - 1.    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    y_true : array, shape = [n_samples]\n",
    "        True binary labels.\n",
    "        \n",
    "    y_score : array, shape = [n_samples]\n",
    "        Target scores, can either be probability estimates of the positive class,\n",
    "        confidence values, or non-thresholded measure of decisions (as returned by\n",
    "        “decision_function” on some classifiers).\n",
    "\n",
    "    === Example ===\n",
    "    y = [0,0,0,1,1,1]\n",
    "    yhat = [0.3,0.6,0.4,.7,.9,.8]\n",
    "    Youden_index(y, yhat)\n",
    "\n",
    "    References\n",
    "    ----------    \n",
    "    Ewald, B. (2006). Post hoc choice of cut points introduced bias to diagnostic research.\n",
    "    Journal of clinical epidemiology, 59(8), 798-801.\n",
    "    \n",
    "    Steyerberg, E.W., Van Calster, B., & Pencina, M.J. (2011). Performance measures for\n",
    "    prediction models and markers: evaluation of predictions and classifications.\n",
    "    Revista Espanola de Cardiologia (English Edition), 64(9), 788-794.\n",
    "    \n",
    "    Jiménez-Valverde, A., & Lobo, J.M. (2007). Threshold criteria for conversion of probability\n",
    "    of species presence to either–or presence–absence. Acta oecologica, 31(3), 361-369.\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    idx = np.argmax(tpr - fpr)\n",
    "    return thresholds[idx]\n",
    "\n",
    "import nets\n",
    "class PVC_NET(pl.LightningModule):\n",
    "    def __init__(self,hyperparameters):\n",
    "        super(PVC_NET, self).__init__()\n",
    "        \n",
    "        self.hyperparameters = hyperparameters\n",
    "        self.experiment_name = str(self.hyperparameters).replace(\"{\",\"\").replace(\"}\",\"\").replace(\"'\",\"\").replace(\": \",\"\").replace(\", \",\"_\").split('_project')[0] # cut name as it is too long\n",
    "        path = f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/\"\n",
    "        print(f'saving path : {path}') \n",
    "        os.makedirs(path, mode=0o777, exist_ok=True)\n",
    "        \n",
    "        self.srTarget = self.hyperparameters['srTarget']\n",
    "        self.featureLength= self.hyperparameters['featureLength']\n",
    "        self.thresholdRPeak = self.hyperparameters['thresholdRPeak'] # 0.7\n",
    "        self.learning_rate = self.hyperparameters['learning_rate']\n",
    "        self.youden_index = 0.5\n",
    "        self.testPlot = False\n",
    "        \n",
    "        # define model using hyperparamters\n",
    "        if 'efficient' in hyperparameters['modelName'] or 'resnet' in hyperparameters['modelName']:\n",
    "            self.net = nets.UNet(modelName = hyperparameters['modelName'], \n",
    "                            spatial_dims = hyperparameters['spatial_dims'],\n",
    "                            in_channels = hyperparameters['inChannels'],\n",
    "                            out_channels = hyperparameters['outChannels'],\n",
    "                            norm = hyperparameters['norm'],\n",
    "                            upsample = hyperparameters['upsample'],\n",
    "                            dropout = hyperparameters['dropout'],\n",
    "                            supervision = hyperparameters['supervision'],\n",
    "                            skipModule = hyperparameters['skipModule'],\n",
    "                            skipASPP =  hyperparameters['skipASPP'],\n",
    "                            se_module= hyperparameters['se'],\n",
    "                           )\n",
    "            \n",
    "#         elif 'U2' in hyperparameters['modelName']:\n",
    "#             self.net = nets.U2NET(in_ch=hyperparameters['in_channels'],\n",
    "#                                   out_ch=hyperparameters['out_channels'],\n",
    "#                                   nnblock = hyperparameters['nnblock'],\n",
    "#                                   ASPP = hyperparameters['ASPP'],\n",
    "#                                   FFC = hyperparameters['FFC'],\n",
    "#                                   acm = hyperparameters['acm'],\n",
    "#                                   dropout = hyperparameters['dropout'],\n",
    "#                                   temperature=1,\n",
    "#                                   norm = hyperparameters['norm'],\n",
    "#                                  )\n",
    "            \n",
    "#         elif 'unetr' in hyperparameters['modelName']:\n",
    "#             self.net= monai.networks.nets.UNETR(hyperparameters['in_channels'], \n",
    "#                                                 hyperparameters['out_channels'], \n",
    "#                                                 2048,\n",
    "#                                                 feature_size = 16,\n",
    "#                                                 hidden_size = 768,\n",
    "#                                                 mlp_dim = 3072,\n",
    "#                                                 num_heads = 12,\n",
    "#                                                 pos_embed = 'conv',\n",
    "#                                                 norm_name= hyperparameters['norm'],\n",
    "#                                                 conv_block = True,\n",
    "#                                                 res_block = True,\n",
    "#                                                 dropout_rate = 0.0,\n",
    "#                                                 spatial_dims = hyperparameters['spatial_dims'],)\n",
    "            \n",
    "        # define loss using hyperparameters\n",
    "        if hyperparameters['lossFn']=='bceloss':\n",
    "            self.lossFn = nn.BCELoss()\n",
    "        elif hyperparameters['lossFn']=='wbceloss':\n",
    "            self.lossFn = BCELoss_class_weighted([.2, .8])\n",
    "        elif hyperparameters['lossFn']=='diceceloss':\n",
    "            self.lossFn = monai.losses.DiceCELoss()\n",
    "        elif hyperparameters['lossFn']=='focalloss':\n",
    "            self.lossFn = FocalLoss()\n",
    "        elif hyperparameters['lossFn']=='dicefocalloss':\n",
    "            self.lossFn = monai.losses.DiceFocalLoss()\n",
    "        elif hyperparameters['lossFn']=='weightedfocalloss':\n",
    "            self.lossFn = WeightedFocalLoss()\n",
    "        elif hyperparameters['lossFn']=='propotionalLoss':\n",
    "            self.lossFn = PropotionalLoss(per_image=False, smooth=1e-7, beta=0.7, bce=True)\n",
    "            \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def compute_loss(self, yhat, y):\n",
    "        if isinstance(yhat,list) or isinstance(yhat,tuple):\n",
    "            yhat, loss_dp = yhat\n",
    "            loss = self.lossFn(yhat,y)\n",
    "            loss = loss + loss_dp\n",
    "        else:\n",
    "            loss = self.lossFn(yhat, y)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        result = self.net(x)\n",
    "        return result\n",
    "        \n",
    "    def sliding_window_inference(self, x): # Inference Sliding window using MONAI API: Using this only valid and test when size of input is larger than 2048\n",
    "        def predictor(x, return_idx = 0): # in case of network gets multiple output, we will use only 1st output\n",
    "            result = self.forward(x)\n",
    "            if isinstance(result, list) or isinstance(result, tuple):\n",
    "                return result[return_idx]\n",
    "            else:\n",
    "                return result        \n",
    "        return sliding_window_inference(x, self.featureLength, 8, predictor, mode='gaussian', overlap=0.75)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        # return {'optimizer': optimizer}\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, min_lr=1e-6)\n",
    "        # scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-3, pct_start=0.02, total_steps=self.trainer.estimated_stepping_batches)\n",
    "        return {'optimizer': optimizer,\n",
    "                'lr_scheduler': {'scheduler': scheduler, 'monitor': 'val_loss'}}\n",
    "    \n",
    "    def pipeline(self, batch, batch_idx):\n",
    "        \n",
    "        x = batch['signal'].float()\n",
    "        y = batch['y_seg'].float()\n",
    "        \n",
    "        dataSource = batch['dataSource'][0]\n",
    "        \n",
    "        fname = batch['fname']\n",
    "        pid = batch['pid']\n",
    "        time = batch['time']\n",
    "        \n",
    "        yhat = self.sliding_window_inference(x) if x.shape[-1] > self.featureLength else self.forward(x)\n",
    "        loss = self.compute_loss(yhat, y)\n",
    "\n",
    "        if isinstance(yhat,tuple) or isinstance(yhat,list): # in case multi output model such as U2NET while training\n",
    "            yhat = yhat[0]\n",
    "                \n",
    "        return {'loss':loss, \"x\": x, \"y\": y, \"yhat\":yhat, \"dataSource\":dataSource,'fname':fname}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):        \n",
    "        result = self.pipeline(batch, batch_idx)\n",
    "        self.log('loss', result['loss'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"loss\":result['loss'], \"x\": result['x'], \"y\": result['y'], \"yhat\":result['yhat'], \"dataSource\":result['dataSource'], 'fname':result['fname']}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        result = self.pipeline(batch, batch_idx)\n",
    "        self.log('val_loss', result['loss'], on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {\"val_loss\":result['loss'], \"x\": result['x'], \"y\": result['y'], \"yhat\":result['yhat'], \"dataSource\":result['dataSource'], 'fname':result['fname']}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        result = self.pipeline(batch, batch_idx)\n",
    "        self.log('test_loss', result['loss'], on_step=False, on_epoch=True)\n",
    "        return {\"test_loss\":result['loss'], \"x\": result['x'], \"y\": result['y'], \"yhat\":result['yhat'], \"dataSource\":result['dataSource'], 'fname':result['fname']}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \n",
    "        self.fnames = []\n",
    "        data = valid_data\n",
    "        for d in data:\n",
    "            self.fnames.append(f\"{d['pid']}_{d['time']}\")\n",
    "\n",
    "        self.evaluations(outputs,'val', False)\n",
    "\n",
    "    def test_epoch_end(self, outputs):        \n",
    "        \n",
    "        self.fnames = []\n",
    "        if outputs[0]['dataSource'][0]==3:\n",
    "            self.dataSource = 'testMIT'\n",
    "            data = test_data\n",
    "        elif outputs[0]['dataSource'][0]==11:\n",
    "            self.dataSource = 'testAMC'\n",
    "            data = AMC_data\n",
    "        elif outputs[0]['dataSource'][0]==12:\n",
    "            self.dataSource = 'testCPSC2020'\n",
    "            data = CPSC2020_data\n",
    "        elif outputs[0]['dataSource'][0]==13:\n",
    "            self.dataSource = 'testINCART'\n",
    "            data = INCART_data\n",
    "        \n",
    "        for d in data:\n",
    "            self.fnames.append(f\"{d['pid']}_{d['time']}\")\n",
    "\n",
    "        self.evaluations(outputs, self.dataSource, True)\n",
    "    \n",
    "    def apply_threshold(self, pred, t):\n",
    "        try:\n",
    "            result = pred.clone()\n",
    "        except:\n",
    "            result = pred.copy()\n",
    "        result[result>=t]= 1\n",
    "        result[result<t]= 0\n",
    "        return result\n",
    "    \n",
    "    def evaluations(self, outputs, dataSource, plot=False):\n",
    "        \n",
    "        # fnames = []\n",
    "        xs = []\n",
    "        ys= []\n",
    "        yhatsRaw = []                    \n",
    "        yhatsRefined = []                    \n",
    "        \n",
    "        ysClass1Raw = []\n",
    "        ysClass2Raw = []\n",
    "        ysClass3Raw = []\n",
    "        \n",
    "        yhatsClass1Raw = []\n",
    "        yhatsClass2Raw = []\n",
    "        yhatsClass3Raw = []\n",
    "        \n",
    "        ysClass1Refined = []\n",
    "        ysClass2Refined = []\n",
    "        ysClass3Refined = []\n",
    "        \n",
    "        yhatsClass1Refined = []\n",
    "        yhatsClass2Refined = []\n",
    "        yhatsClass3Refined = []\n",
    "\n",
    "        RRaw_TP = 0\n",
    "        RRaw_FP = 0\n",
    "        RRaw_FN = 0\n",
    "        RRefined_TP = 0\n",
    "        RRefined_FP = 0\n",
    "        RRefined_FN = 0\n",
    "        \n",
    "        for output in outputs:\n",
    "            # fnames.extend(output['fname'])\n",
    "            xs.extend(output[\"x\"].cpu().detach().numpy())\n",
    "            ys.extend(output[\"y\"].cpu().detach().numpy())\n",
    "\n",
    "            for i in range(len(output[\"y\"])):\n",
    "                y = output[\"y\"][i].cpu().detach().numpy()\n",
    "                yhatRaw = output[\"yhat\"][i].cpu().detach().numpy()\n",
    "                yhatRefined = self.postProcessByRPeak(output[\"yhat\"][i].cpu().detach().numpy())\n",
    "                \n",
    "                yhatsRaw.append(yhatRaw)\n",
    "                yhatsRefined.append(yhatRefined)\n",
    "                \n",
    "                yhatRaw_eval = self.eval_Peak(yhatRaw, y)                \n",
    "                RRaw_TP += yhatRaw_eval['R_TPs']\n",
    "                RRaw_FP += yhatRaw_eval['R_FPs']\n",
    "                RRaw_FN += yhatRaw_eval['R_FNs']\n",
    "                ysClass1Raw.extend(yhatRaw_eval['ys_class1'])\n",
    "                yhatsClass1Raw.extend(yhatRaw_eval['yhats_class1'])                \n",
    "                ysClass2Raw.extend(yhatRaw_eval['ys_class2'])\n",
    "                yhatsClass2Raw.extend(yhatRaw_eval['yhats_class2'])                \n",
    "                ysClass3Raw.extend(yhatRaw_eval['ys_class3'])\n",
    "                yhatsClass3Raw.extend(yhatRaw_eval['yhats_class3'])                \n",
    "\n",
    "                yhatRefined_eval = self.eval_Peak(yhatRefined, y)                \n",
    "                RRefined_TP += yhatRefined_eval['R_TPs']\n",
    "                RRefined_FP += yhatRefined_eval['R_FPs']\n",
    "                RRefined_FN += yhatRefined_eval['R_FNs']\n",
    "                ysClass1Refined.extend(yhatRefined_eval['ys_class1'])\n",
    "                yhatsClass1Refined.extend(yhatRefined_eval['yhats_class1'])\n",
    "                ysClass2Refined.extend(yhatRefined_eval['ys_class2'])\n",
    "                yhatsClass2Refined.extend(yhatRefined_eval['yhats_class2'])\n",
    "                ysClass3Refined.extend(yhatRefined_eval['ys_class3'])\n",
    "                yhatsClass3Refined.extend(yhatRefined_eval['yhats_class3'])\n",
    "                \n",
    "        del output\n",
    "        \n",
    "        fnames = np.array(self.fnames)\n",
    "        xs = np.array(xs)\n",
    "        ys = np.array(ys)\n",
    "        yhatsRaw = np.array(yhatsRaw)\n",
    "        yhatsRefined = np.array(yhatsRefined)\n",
    "        \n",
    "        ysClass1Raw = np.array(ysClass1Raw)\n",
    "        ysClass2Raw = np.array(ysClass2Raw)\n",
    "        ysClass3Raw = np.array(ysClass3Raw)\n",
    "        ysClass1Refined = np.array(ysClass1Refined)\n",
    "        ysClass2Refined = np.array(ysClass2Refined)\n",
    "        ysClass3Refined = np.array(ysClass3Refined)\n",
    "        yhatsClass1Raw = np.array(yhatsClass1Raw)\n",
    "        yhatsClass2Raw = np.array(yhatsClass2Raw)\n",
    "        yhatsClass3Raw = np.array(yhatsClass3Raw)\n",
    "        yhatsClass1Refined = np.array(yhatsClass1Refined)\n",
    "        yhatsClass2Refined = np.array(yhatsClass2Refined)\n",
    "        yhatsClass3Refined = np.array(yhatsClass3Refined)\n",
    "        \n",
    "#         print('Raw',ys.shape,yhatsRaw.shape,yhatsRefined.shape)\n",
    "#         print('picked',ysClass1Raw.shape,yhatsClass1Raw.shape,ysClass1Refined.shape,yhatsClass1Refined.shape,\n",
    "#                       ysClass2Raw.shape,yhatsClass2Raw.shape,ysClass2Refined.shape,yhatsClass2Refined.shape,\n",
    "#                       ysClass3Raw.shape,yhatsClass3Raw.shape,ysClass3Refined.shape,yhatsClass3Refined.shape)\n",
    "        \n",
    "        def eval_cm(ys, yhats, name):\n",
    "            \"\"\"\n",
    "            input shape shoud be only [ B ]\n",
    "            \"\"\"\n",
    "            \n",
    "            TP = 0\n",
    "            FN = 0\n",
    "            FP = 0\n",
    "            TN = 0\n",
    "\n",
    "            auc = sklearn.metrics.roc_auc_score(ys, yhats)\n",
    "            ap  = sklearn.metrics.average_precision_score(ys, yhats)\n",
    "            self.youden_index = Youden_index(ys, yhats)\n",
    "            \n",
    "            negativeIdx = np.where(ys == 0)\n",
    "            positiveIdx = np.where(ys != 0)\n",
    "            \n",
    "            for i in range(len(negativeIdx[0])):\n",
    "                z = negativeIdx[0][i]\n",
    "                FP = FP+1 if yhats[z]>=self.youden_index else FP\n",
    "                TN = TN+1 if yhats[z]<self.youden_index else TN\n",
    "            for i in range(len(positiveIdx[0])):\n",
    "                n = positiveIdx[0][i]\n",
    "                TP = TP+1 if yhats[n]>=self.youden_index else TP\n",
    "                FN = FN+1 if yhats[n]<self.youden_index else FN\n",
    "            \n",
    "            # print(TP,TN,FP,FN)\n",
    "            sen = TP/(TP+FN)\n",
    "            spe = TN/(TN+FP)\n",
    "            acc = (TP+TN)/(TP+FN+FP+TN)\n",
    "            bacc = (sen+spe)/2\n",
    "            \n",
    "            # f1 = sklearn.metrics.f1_score(gt1, apply_threshold(yhat1,yi))\n",
    "            # pr  = sklearn.metrics.precision_score(gt1,apply_threshold(yhat1,yi))\n",
    "            f1 = 2*TP/(2*TP+FP+FN)\n",
    "            ppv = TP/(TP+FP)\n",
    "            npv = TN/(TN+FN)\n",
    "\n",
    "            # plot performance\n",
    "            plt.figure(figsize=(20,4))\n",
    "            plt.subplot(131)           \n",
    "            plt.title(f'Histogram of likelihood (Youden index : {self.youden_index:.3f})')\n",
    "            plt.xlabel('Likelihood')\n",
    "            plt.ylabel('Normalized samples')\n",
    "            plt.hist(yhats[negativeIdx],bins=50,density=True,label='Likelihood of Negative Cases',alpha=0.5)\n",
    "            plt.hist(yhats[positiveIdx],bins=50,density=True,label='Likelihood of Positive Cases',alpha=0.5)\n",
    "            plt.vlines(self.youden_index,0,10,label='Youden index',color='r',alpha=0.5)\n",
    "            plt.xlim([0,1])\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(132)\n",
    "            plt.title(f'ROC (AUROC : {auc:.3f})')\n",
    "            plt.xlabel('1-Specificity')\n",
    "            plt.ylabel('Sensitivity')\n",
    "            fpr, tpr, _ = roc_curve(ys, yhats)\n",
    "            plt.plot(fpr, tpr)\n",
    "            \n",
    "            plt.subplot(133)        \n",
    "            plt.title(f'Precision-Recall (AP : {ap:.3f})')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            prec, recall, _ = precision_recall_curve(ys, yhats)\n",
    "            plt.plot(recall, prec)\n",
    "            # plt.show()\n",
    "            os.makedirs(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/metric/\", mode=0o777, exist_ok=True)\n",
    "            plt.savefig(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/metric/performance_{dataSource}_{name}.png\")\n",
    "            plt.close()\n",
    "            \n",
    "            return {\"TP\":TP,\"TN\":TN,\"FN\":FN,\"FP\":FP,\n",
    "                    \"auc\":auc,\"ap\":ap,\"sen\":sen,\"spe\":spe,\"acc\":acc,\"bacc\":bacc,\"f1\":f1,\"ppv\":ppv,\"npv\":npv,\"youdenIndex\":self.youden_index}\n",
    "        \n",
    "        try:\n",
    "            cmClass1Raw = eval_cm(ysClass1Raw, yhatsClass1Raw,'Class1Raw')\n",
    "            cmClass1Refined = eval_cm(ysClass1Refined, yhatsClass1Refined,'Class1Refined')\n",
    "            cmClass2Raw = eval_cm(ysClass2Raw, yhatsClass2Raw,'Class2Raw')\n",
    "            cmClass2Refined = eval_cm(ysClass2Refined, yhatsClass2Refined,'Class2Refined')\n",
    "            cmClass3Raw = eval_cm(ysClass3Raw, yhatsClass3Raw,'Class3Raw')\n",
    "            cmClass3Refined = eval_cm(ysClass3Refined, yhatsClass3Refined,'Class3Refined')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        RRaw_sen = RRaw_TP/(RRaw_TP+RRaw_FN)\n",
    "        RRaw_pp  = RRaw_TP/(RRaw_TP+RRaw_FP)\n",
    "        RRaw_err = (RRaw_FP+RRaw_FN)/(RRaw_TP+RRaw_FP+RRaw_FN)\n",
    "\n",
    "        RRefined_sen = RRefined_TP/(RRefined_TP+RRefined_FN)\n",
    "        RRefined_pp  = RRefined_TP/(RRefined_TP+RRefined_FP)\n",
    "        RRefined_err = (RRefined_FP+RRefined_FN)/(RRefined_TP+RRefined_FP+RRefined_FN)\n",
    "            \n",
    "        def logcmResult(cmPVC, refined):\n",
    "            \n",
    "            self.log(f'{dataSource}_TP_{refined}',cmPVC['TP'],on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_FN_{refined}',cmPVC['FN'],on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_FP_{refined}',cmPVC['FP'],on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_TN_{refined}',cmPVC['TN'],on_step=False,on_epoch=True)\n",
    "\n",
    "            self.log(f'{dataSource}_AUPRC_{refined}',cmPVC['ap'],on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_AUROC_{refined}',cmPVC['auc'],on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_ACC_{refined}',cmPVC['acc'],on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_SEN_{refined}',cmPVC['sen'],on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_SPE_{refined}',cmPVC['spe'],on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_BACC_{refined}',cmPVC['bacc'],on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_F1_{refined}',cmPVC['f1'],on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_PPV_{refined}',cmPVC['ppv'],on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_NPV_{refined}',cmPVC['npv'],on_step=False,on_epoch=True)\n",
    "            \n",
    "            self.log(f'{dataSource}_YoudenIndex_{refined}',cmPVC['youdenIndex'],on_step=False,on_epoch=True)\n",
    "            \n",
    "        def logRResult(R_sen,R_ppv,R_der, refined):\n",
    "            self.log(f'{dataSource}_R-DER_{refined}',R_der,on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_R-PPV_{refined}',R_ppv,on_step=False,on_epoch=True)\n",
    "            self.log(f'{dataSource}_R-SEN_{refined}',R_sen,on_step=False,on_epoch=True)\n",
    "        \n",
    "        try:\n",
    "            logcmResult(cmClass1Raw,'Class1Raw')\n",
    "            logcmResult(cmClass1Refined,'Class1Refined')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            logcmResult(cmClass2Raw,'Class2Raw')\n",
    "            logcmResult(cmClass2Refined,'Class2Refined')\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            logcmResult(cmClass3Raw,'Class3Raw')\n",
    "            logcmResult(cmClass3Refined,'Class3Refined')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        logRResult(RRaw_sen,RRaw_pp,RRaw_err, 'Raw')\n",
    "        logRResult(RRefined_sen,RRefined_pp,RRefined_err,'Refined')\n",
    "        \n",
    "        os.makedirs(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/metric/\", mode=0o777, exist_ok=True)\n",
    "\n",
    "        if len(ysClass3Raw)!=0 and len(ysClass2Raw)!=0:\n",
    "            df = pd.DataFrame([ysClass1Raw, yhatsClass1Raw, ysClass2Raw, yhatsClass2Raw, ysClass3Raw, yhatsClass3Raw],\n",
    "                              ['ysClass1Raw','yhatsClass1Raw','ysClass2Raw','yhatsClass2Raw','ysClass3Raw','yhatsClass3Raw'])\n",
    "            df = df.T\n",
    "            df.to_csv(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/metric/likelihood_{dataSource}_Raw.csv\",index=False)\n",
    "\n",
    "            df = pd.DataFrame([ysClass1Refined, yhatsClass1Refined,ysClass2Refined, yhatsClass2Refined,ysClass3Refined, yhatsClass3Refined],\n",
    "                              ['ysClass1Refined','yhatsClass1Refined','ysClass2Refined','yhatsClass2Refined','ysClass3Refined','yhatsClass3Refined'])\n",
    "            df = df.T\n",
    "            df.to_csv(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/metric/likelihood_{dataSource}_Refined.csv\",index=False)\n",
    "        elif len(ysClass2Raw)!=0:\n",
    "            df = pd.DataFrame([ysClass1Raw, yhatsClass1Raw, ysClass2Raw, yhatsClass2Raw],['ysClass1Raw','yhatsClass1Raw','ysClass2Raw','yhatsClass2Raw'])\n",
    "            df = df.T\n",
    "            df.to_csv(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/metric/likelihood_{dataSource}_Raw.csv\",index=False)\n",
    "\n",
    "            df = pd.DataFrame([ysClass1Refined, yhatsClass1Refined,ysClass2Refined, yhatsClass2Refined],\n",
    "                              ['ysClass1Refined','yhatsClass1Refined','ysClass2Refined','yhatsClass2Refined'])\n",
    "            df = df.T\n",
    "            df.to_csv(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/metric/likelihood_{dataSource}_Refined.csv\",index=False)\n",
    "        else:\n",
    "            df = pd.DataFrame([ysClass1Raw, yhatsClass1Raw],['ysClass1Raw','yhatsClass1Raw'])\n",
    "            df = df.T\n",
    "            df.to_csv(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/metric/likelihood_{dataSource}_Raw.csv\",index=False)\n",
    "\n",
    "            df = pd.DataFrame([ysClass1Refined, yhatsClass1Refined],['ysClass1Refined','yhatsClass1Refined'])\n",
    "            df = df.T\n",
    "            df.to_csv(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/metric/likelihood_{dataSource}_Refined.csv\",index=False)\n",
    "                \n",
    "        for i in range(len(yhatsRefined)):\n",
    "            yhatsRefined[i] = self.postProcessByYoudenIndex(yhatsRefined[i], cmClass1Refined['youdenIndex'])\n",
    "        # try:\n",
    "        #     yhatsRefined[:,1] = self.postProcessByYoudenIndex(yhatsRefined[:,1], cmClass1Refined['youdenIndex'])\n",
    "        # except:\n",
    "        #     pass\n",
    "        self.plotCaseResult(xs, ys, yhatsRaw, self.apply_threshold(yhatsRefined,cmClass1Raw['youdenIndex']), fnames) if self.testPlot else 0\n",
    "\n",
    "    def eval_Peak(self, yhat, y):\n",
    "        \n",
    "        \"\"\"\n",
    "        input y: CxSignal\n",
    "        input yhat : CXSignal\n",
    "        \"\"\"\n",
    "        classes = yhat.shape[0]-1\n",
    "\n",
    "        try:\n",
    "            yhat_ = yhat.clone()\n",
    "        except:\n",
    "            yhat_ = yhat.copy()\n",
    "            \n",
    "        yhat_[0] = self.apply_threshold(yhat_[0],self.thresholdRPeak)\n",
    "            \n",
    "        ys_class1 = []\n",
    "        ys_class2 = []\n",
    "        ys_class3 = []\n",
    "        yhats_class1 = []\n",
    "        yhats_class2 = []\n",
    "        yhats_class3 = []\n",
    "\n",
    "        # evalutation of R-peak\n",
    "        R_TP = []\n",
    "        R_FP = []\n",
    "        R_FN = []\n",
    "        \n",
    "        result_y, count_y = label(y[0])\n",
    "\n",
    "        for j in range(1, count_y+1):\n",
    "            index = np.where(result_y == j)[0]\n",
    "            start = index[0]\n",
    "            end = index[-1]\n",
    "            \n",
    "            try:\n",
    "                yhat0_mean = torch.nanmean(yhat_[0,start:end+1])\n",
    "                yhat1_mean = torch.nanmean(yhat_[1,start:end+1])\n",
    "                yhat2_mean = torch.nanmean(yhat_[2,start:end+1])\n",
    "                yhat3_mean = torch.nanmean(yhat_[3,start:end+1])\n",
    "            except:\n",
    "                try:\n",
    "                    yhat0_mean = np.nanmean(yhat_[0,start:end+1])\n",
    "                    yhat1_mean = np.nanmean(yhat_[1,start:end+1])\n",
    "                    yhat2_mean = np.nanmean(yhat_[2,start:end+1])\n",
    "                    yhat3_mean = np.nanmean(yhat_[3,start:end+1])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            # evalutation of R-peak : TP, FN\n",
    "            if 1 in y[0,start:end+1] and yhat0_mean>=self.thresholdRPeak:\n",
    "                R_TP.append(1)\n",
    "            elif 1 in y[0,start:end+1] and yhat0_mean<self.thresholdRPeak:\n",
    "                R_FN.append(1)            \n",
    "                \n",
    "            # evalutation of PVC : just return likelihood\n",
    "            if 0 in y[1,start:end]:\n",
    "                ys_class1.append(0)\n",
    "                yhats_class1.append(yhat1_mean)\n",
    "            elif 1 in y[1,start:end]:\n",
    "                ys_class1.append(1)\n",
    "                yhats_class1.append(yhat1_mean)\n",
    "                \n",
    "            try:\n",
    "                if 0 in y[2,start:end]:\n",
    "                    ys_class2.append(0)\n",
    "                    yhats_class2.append(yhat2_mean)\n",
    "                elif 1 in y[2,start:end]:\n",
    "                    ys_class2.append(1)\n",
    "                    yhats_class2.append(yhat2_mean)\n",
    "\n",
    "                if 0 in y[3,start:end]:\n",
    "                    ys_class3.append(0)\n",
    "                    yhats_class3.append(yhat3_mean)\n",
    "                elif 1 in y[3,start:end]:\n",
    "                    ys_class3.append(1)\n",
    "                    yhats_class3.append(yhat3_mean)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        # print('B',count_y,np.array(ys).shape, np.array(yhats).shape, len(R_TP),len(R_FP),len(R_FN))\n",
    "\n",
    "        result_yhat, count_yhat = label(yhat_[0])\n",
    "\n",
    "        for j in range(1,count_yhat+1):\n",
    "            index = np.where(result_yhat == j)[0]\n",
    "            start = index[0]\n",
    "            end = index[-1]\n",
    "            \n",
    "            try:\n",
    "                yhat0_mean = torch.nanmean(yhat_[0,start:end+1])\n",
    "                yhat1_mean = torch.nanmean(yhat_[1,start:end+1])\n",
    "                yhat2_mean = torch.nanmean(yhat_[2,start:end+1])\n",
    "                yhat3_mean = torch.nanmean(yhat_[3,start:end+1])\n",
    "            except:                \n",
    "                try:\n",
    "                    yhat0_mean = np.nanmean(yhat_[0,start:end+1])\n",
    "                    yhat1_mean = np.nanmean(yhat_[1,start:end+1])\n",
    "                    yhat2_mean = np.nanmean(yhat_[2,start:end+1])\n",
    "                    yhat3_mean = np.nanmean(yhat_[3,start:end+1])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "\n",
    "            # evalutation of R-peak : FP\n",
    "            if 1 not in y[0,start:end+1]:\n",
    "                R_FP.append(1)\n",
    "\n",
    "            # evalutation of PVC : FP\n",
    "            if 1 not in y[1,start:end+1] and 1 in yhat_[1,start:end+1]:\n",
    "                ys_class1.append(0)\n",
    "                yhats_class1.append(yhat1_mean)\n",
    "            \n",
    "            try:\n",
    "                if 1 not in y[2,start:end+1] and 1 in yhat_[2,start:end+1]:\n",
    "                    ys_class2.append(0)\n",
    "                    yhats_class2.append(yhat2_mean)\n",
    "                    \n",
    "                if 1 not in y[3,start:end+1] and 1 in yhat_[3,start:end+1]:\n",
    "                    ys_class3.append(0)\n",
    "                    yhats_class3.append(yhat3_mean)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return {\n",
    "                'R_TPs':np.sum(R_TP), 'R_FNs':np.sum(R_FN), 'R_FPs':np.sum(R_FP),\n",
    "                'ys_class1':np.array(ys_class1), 'yhats_class1':np.array(yhats_class1),\n",
    "                'ys_class2':np.array(ys_class2), 'yhats_class2':np.array(yhats_class2),\n",
    "                'ys_class3':np.array(ys_class3), 'yhats_class3':np.array(yhats_class3)\n",
    "               }\n",
    "    \n",
    "    def postProcessByRPeak(self, yhat):\n",
    "        \"\"\"\n",
    "        input : yhat [C x S]\n",
    "        output : yhat [C x S]\n",
    "        \n",
    "        Rule 0. R-peak는 self.thresholdRPeak로 binarize한다.\n",
    "        Rule 1. R-peak의 간격이 특정 간격보다 작으면 무시한다. \n",
    "        Rule 2. R-peak가 아니면 PVC, AFIB도 아니다. # R-peak에 살짝 마진을 주고 PVC, AFIB과 곱해준다\n",
    "        threshold = int(srTarget*.05)\n",
    "        \"\"\"\n",
    "\n",
    "        yhat_ = yhat.copy()\n",
    "        # Rule 0.\n",
    "        yhat_[0] = self.apply_threshold(yhat_[0], self.thresholdRPeak)\n",
    "        \n",
    "        # Rule 1. fill in and remove R- peak\n",
    "        threshold_hole = int(self.srTarget*.2*0.2) # 20% of R-peak \n",
    "        yhat_[0] = morphology.remove_small_holes(yhat_[0].astype(bool), threshold_hole).astype(float) # fill in\n",
    "        \n",
    "        threshold_object = int(self.srTarget*.2*0.7) # 70% of R-peak seg\n",
    "        yhat_[0] = morphology.remove_small_objects(yhat_[0].astype(bool), threshold_object).astype(float) # remove small R-peak\n",
    "        yhat_0_dilated = ndimage.binary_dilation(yhat_[0],iterations=int(self.srTarget*.1*0.1))         #dilated\n",
    "        \n",
    "        # Rule 2.\n",
    "        yhat_[1] = yhat_0_dilated*yhat_[1] \n",
    "        try:\n",
    "            yhat_[2] = yhat_0_dilated*yhat_[2] \n",
    "            yhat_[3] = yhat_0_dilated*yhat_[3] \n",
    "        except:\n",
    "            pass\n",
    "        return yhat_\n",
    "    \n",
    "    def postProcessByYoudenIndex(self, yhat, threshold):\n",
    "        \"\"\"\n",
    "        input : yhat [C x S]\n",
    "        output : yhat [C x S]        \n",
    "        \"\"\"\n",
    "        result, count_yhat = label(yhat[0])\n",
    "        for j in range(1, count_yhat+1):\n",
    "            index = np.where(result == j)[0]\n",
    "            start = index[0]\n",
    "            end = index[-1]\n",
    "            margin = int(self.srTarget*.2*.1)\n",
    "            \n",
    "            yhat1_mean = np.nanmean(yhat[1,start:end+1])\n",
    "            \n",
    "            # evalutation of PVC : FP\n",
    "            if yhat1_mean >= threshold:\n",
    "                yhat[1,start-margin:end+1+margin] = 1\n",
    "            else:\n",
    "                yhat[1,start-margin:end+1+margin] = 0\n",
    "                \n",
    "        yhat = yhat.round()\n",
    "        return yhat\n",
    "    \n",
    "    def plotCaseResult(self, x, y, yhat1, yhat2, fname):\n",
    "        t = np.linspace(0,x.shape[-1]/self.srTarget, x.shape[-1]) # for x-axis ticks\n",
    "        \n",
    "        for idx in range(len(y)):\n",
    "            plt.figure(figsize=(20,12))\n",
    "            plt.subplot(221)\n",
    "            plt.title(f'Prediction result of {self.dataSource}')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Normalized ECG')\n",
    "            plt.plot(t,x[idx,0],alpha=0.9,color='black',label='ECG signal')\n",
    "            plt.plot(t,yhat1[idx,0],alpha=0.7,color='b',label='R Peak prediction (Likelihood)')\n",
    "            plt.plot(t,yhat1[idx,1],alpha=0.7,color='r',label='PVC prediction (Likelihood)')\n",
    "            # other annotation            \n",
    "            try:\n",
    "                plt.plot(t,yhat1[idx,2],alpha=0.7,color='g',label='AFIB prediction (Likelihood)')\n",
    "                plt.plot(t,yhat1[idx,3],alpha=0.7,color='orange',label='Others prediction (Likelihood)')\n",
    "            except:\n",
    "                pass\n",
    "            plt.xticks(np.arange(0, len(t)/self.srTarget, step=1))\n",
    "            plt.ylim([0,1.5])\n",
    "            plt.legend(loc=1)\n",
    "            \n",
    "            plt.subplot(222)\n",
    "            plt.title(f'Raw signal of {self.dataSource}')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Normalized ECG')\n",
    "            plt.plot(t,x[idx,0],alpha=0.9,color='black',label='ECG signal')\n",
    "            plt.xticks(np.arange(0, len(t)/self.srTarget, step=1))\n",
    "            plt.ylim([0,1.5])\n",
    "            plt.legend(loc=1)\n",
    "\n",
    "            \n",
    "            plt.subplot(223)\n",
    "            plt.title(f'Refined Prediction result of {self.dataSource}')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Normalized ECG')\n",
    "            plt.plot(t,x[idx,0],alpha=0.9,color='black',label='ECG signal')\n",
    "            plt.plot(t,yhat2[idx,0],alpha=0.7,color='b',label='R Peak prediction (Binarized)')\n",
    "            plt.plot(t,yhat2[idx,1],alpha=0.7,color='r',label='PVC prediction (Binarized)')\n",
    "            # other annotation\n",
    "            try:\n",
    "                plt.plot(t,ndimage.binary_dilation(yhat2[idx,2],iterations=int(srTarget*.1*0.2)),alpha=0.7,color='g',label='AFIB prediction (Likelihood)')\n",
    "                plt.plot(t,ndimage.binary_dilation(yhat2[idx,3],iterations=int(srTarget*.1*0.3)),alpha=0.7,color='orange',label='Others prediction (Likelihood)')\n",
    "            except:\n",
    "                pass\n",
    "            plt.xticks(np.arange(0, len(t)/self.srTarget, step=1))\n",
    "            plt.ylim([0,1.5])\n",
    "            plt.legend(loc=1)\n",
    "\n",
    "            plt.subplot(224)\n",
    "            plt.title(f'Ground truth of {self.dataSource}')\n",
    "            plt.xlabel('Time (s)')\n",
    "            plt.ylabel('Normalized ECG')\n",
    "            plt.plot(t,x[idx,0],alpha=0.9,color='black',label='ECG signal')\n",
    "            plt.plot(t,y[idx,0],alpha=0.7,color='b',label='R Peak GT') if y is not None else y\n",
    "            plt.plot(t,y[idx,1],alpha=0.7,color='r',label='PVC GT') if y is not None else y\n",
    "            # other annotation            \n",
    "            try:\n",
    "                plt.plot(t,ndimage.binary_dilation(y[idx,2],iterations=int(srTarget*.1*0.2)),alpha=0.7,color='g',label='AFIB prediction (Likelihood)')\n",
    "                plt.plot(t,ndimage.binary_dilation(y[idx,3],iterations=int(srTarget*.1*0.3)),alpha=0.7,color='orange',label='Others prediction (Likelihood)')\n",
    "            except:\n",
    "                pass\n",
    "            plt.xticks(np.arange(0, len(t)/self.srTarget, step=1))\n",
    "            plt.ylim([0,1.5])\n",
    "            plt.legend(loc=1)\n",
    "            plt.tight_layout()\n",
    "            # plt.show()\n",
    "\n",
    "            os.makedirs(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/result/\", mode=0o777, exist_ok=True)\n",
    "            os.makedirs(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/result/{self.dataSource}/\", mode=0o777, exist_ok=True)\n",
    "            plt.savefig(f\"{self.hyperparameters['path_logRoot']}/{self.experiment_name}/result/{self.dataSource}/{str(fname[idx])}.png\")\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882bae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5610 1431 905 14262 497 41753 10811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = np.load('dataset/mit-bih-arrhythmia-database-1.0.0_trainSeg_seed4.npy',allow_pickle=True) # B x (C) x Signal\n",
    "valid_data = np.load('dataset/mit-bih-arrhythmia-database-1.0.0_validSeg_seed4.npy',allow_pickle=True) # B x (C) x Signal\n",
    "test_data = np.load('dataset/mit-bih-arrhythmia-database-1.0.0_testSeg.npy',allow_pickle=True) # B x (C) x Signal\n",
    "\n",
    "Fantasia_data = np.load('dataset/fantasia-database-1.0.0_testSeg.npy', allow_pickle=True) # B x (C) x Signal\n",
    "AMC_data  = np.load('dataset/AMC_PeakLabel_3rd_125Hz.npy',allow_pickle=True) # 497 samples\n",
    "INCART_data  = np.load('dataset/INCART_testSeg.npy',allow_pickle=True)\n",
    "CPSC2020_data  = np.load('dataset/CPSC2020_testSeg.npy',allow_pickle=True)\n",
    "AMCREAL_data = np.load('dataset/AMCREAL_testSeg.npy',allow_pickle=True)\n",
    "print(len(train_data), len(valid_data), len(test_data), len(Fantasia_data), len(AMC_data), len(CPSC2020_data), len(AMCREAL_data))\n",
    "\n",
    "\n",
    "def add_datainfo(data, info_string):\n",
    "    new_data = []\n",
    "    for d in data:\n",
    "        d['dataSource'] = info_string\n",
    "        new_data.append(d)\n",
    "    return np.array(new_data)\n",
    "\n",
    "# add_datainfo(train_data,'train')\n",
    "# add_datainfo(valid_data,'val')\n",
    "# add_datainfo(test_data,'test')\n",
    "\n",
    "# add_datainfo(AMC_data,'AMC')\n",
    "# add_datainfo(CPSC2020_data,'CPSC2020')\n",
    "# add_datainfo(INCART_data,'INCART')\n",
    "# add_datainfo(Fantasia_data,'Fantasia')\n",
    "# add_datainfo(AMCREAL_data,'AMCREAL')\n",
    "\n",
    "add_datainfo(train_data,1)\n",
    "add_datainfo(valid_data,2)\n",
    "add_datainfo(test_data,3)\n",
    "\n",
    "add_datainfo(AMC_data,11)\n",
    "add_datainfo(CPSC2020_data,12)\n",
    "add_datainfo(INCART_data,13)\n",
    "add_datainfo(Fantasia_data,14)\n",
    "add_datainfo(AMCREAL_data,15)\n",
    "print()\n",
    "\n",
    "\n",
    "from warnings import warn\n",
    "import numpy as np\n",
    "\n",
    "from neurokit2.misc import NeuroKitWarning, listify\n",
    "from neurokit2.signal.signal_resample import signal_resample\n",
    "from neurokit2.signal.signal_simulate import signal_simulate\n",
    "\n",
    "def signal_distort(\n",
    "    signal,\n",
    "    sampling_rate=1000,\n",
    "    noise_shape=\"laplace\",\n",
    "    noise_amplitude=0,\n",
    "    noise_frequency=100,\n",
    "    powerline_amplitude=0,\n",
    "    powerline_frequency=50,\n",
    "    artifacts_amplitude=0,\n",
    "    artifacts_frequency=100,\n",
    "    artifacts_number=5,\n",
    "    linear_drift=False,\n",
    "    random_state=None,\n",
    "    silent=False,\n",
    "):\n",
    "    \"\"\"**Signal distortion**\n",
    "\n",
    "    Add noise of a given frequency, amplitude and shape to a signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : Union[list, np.array, pd.Series]\n",
    "        The signal (i.e., a time series) in the form of a vector of values.\n",
    "    sampling_rate : int\n",
    "        The sampling frequency of the signal (in Hz, i.e., samples/second).\n",
    "    noise_shape : str\n",
    "        The shape of the noise. Can be one of ``\"laplace\"`` (default) or\n",
    "        ``\"gaussian\"``.\n",
    "    noise_amplitude : float\n",
    "        The amplitude of the noise (the scale of the random function, relative\n",
    "        to the standard deviation of the signal).\n",
    "    noise_frequency : float\n",
    "        The frequency of the noise (in Hz, i.e., samples/second).\n",
    "    powerline_amplitude : float\n",
    "        The amplitude of the powerline noise (relative to the standard\n",
    "        deviation of the signal).\n",
    "    powerline_frequency : float\n",
    "        The frequency of the powerline noise (in Hz, i.e., samples/second).\n",
    "    artifacts_amplitude : float\n",
    "        The amplitude of the artifacts (relative to the standard deviation of\n",
    "        the signal).\n",
    "    artifacts_frequency : int\n",
    "        The frequency of the artifacts (in Hz, i.e., samples/second).\n",
    "    artifacts_number : int\n",
    "        The number of artifact bursts. The bursts have a random duration\n",
    "        between 1 and 10% of the signal duration.\n",
    "    linear_drift : bool\n",
    "        Whether or not to add linear drift to the signal.\n",
    "    random_state : int\n",
    "        Seed for the random number generator. Keep it fixed for reproducible\n",
    "        results.\n",
    "    silent : bool\n",
    "        Whether or not to display warning messages.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Vector containing the distorted signal.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    .. ipython:: python\n",
    "\n",
    "      import numpy as np\n",
    "      import pandas as pd\n",
    "      import neurokit2 as nk\n",
    "\n",
    "      signal = nk.signal_simulate(duration=10, frequency=0.5)\n",
    "\n",
    "      # Noise\n",
    "      @savefig p_signal_distort1.png scale=100%\n",
    "      noise = pd.DataFrame({\"Freq100\": nk.signal_distort(signal, noise_frequency=200),\n",
    "                           \"Freq50\": nk.signal_distort(signal, noise_frequency=50),\n",
    "                           \"Freq10\": nk.signal_distort(signal, noise_frequency=10),\n",
    "                           \"Freq5\": nk.signal_distort(signal, noise_frequency=5),\n",
    "                           \"Raw\": signal}).plot()\n",
    "      @suppress\n",
    "      plt.close()\n",
    "\n",
    "    .. ipython:: python\n",
    "\n",
    "      # Artifacts\n",
    "      @savefig p_signal_distort2.png scale=100%\n",
    "      artifacts = pd.DataFrame({\"1Hz\": nk.signal_distort(signal, noise_amplitude=0,\n",
    "                                                        artifacts_frequency=1,\n",
    "                                                        artifacts_amplitude=0.5),\n",
    "                               \"5Hz\": nk.signal_distort(signal, noise_amplitude=0,\n",
    "                                                        artifacts_frequency=5,\n",
    "                                                        artifacts_amplitude=0.2),\n",
    "                               \"Raw\": signal}).plot()\n",
    "      @suppress\n",
    "      plt.close()\n",
    "\n",
    "    \"\"\"\n",
    "    # Seed the random generator for reproducible results.\n",
    "    # np.random.seed(random_state)\n",
    "\n",
    "    # Make sure that noise_amplitude is a list.\n",
    "    if isinstance(noise_amplitude, (int, float)):\n",
    "        noise_amplitude = [noise_amplitude]\n",
    "\n",
    "    signal_sd = np.std(signal, ddof=1)\n",
    "    if signal_sd == 0:\n",
    "        signal_sd = None\n",
    "\n",
    "    noise = 0\n",
    "\n",
    "    # Basic noise.\n",
    "    if min(noise_amplitude) > 0:\n",
    "        noise += _signal_distort_noise_multifrequency(\n",
    "            signal,\n",
    "            signal_sd=signal_sd,\n",
    "            sampling_rate=sampling_rate,\n",
    "            noise_amplitude=noise_amplitude,\n",
    "            noise_frequency=noise_frequency,\n",
    "            noise_shape=noise_shape,\n",
    "            silent=silent,\n",
    "        )\n",
    "\n",
    "    # Powerline noise.\n",
    "    if powerline_amplitude > 0:\n",
    "        noise += _signal_distort_powerline(\n",
    "            signal,\n",
    "            signal_sd=signal_sd,\n",
    "            sampling_rate=sampling_rate,\n",
    "            powerline_frequency=powerline_frequency,\n",
    "            powerline_amplitude=powerline_amplitude,\n",
    "            silent=silent,\n",
    "        )\n",
    "\n",
    "    # Artifacts.\n",
    "    if artifacts_amplitude > 0:\n",
    "        noise += _signal_distort_artifacts(\n",
    "            signal,\n",
    "            signal_sd=signal_sd,\n",
    "            sampling_rate=sampling_rate,\n",
    "            artifacts_frequency=artifacts_frequency,\n",
    "            artifacts_amplitude=artifacts_amplitude,\n",
    "            artifacts_number=artifacts_number,\n",
    "            silent=silent,\n",
    "        )\n",
    "\n",
    "    if linear_drift:\n",
    "        noise += _signal_linear_drift(signal)\n",
    "\n",
    "    distorted = signal + noise\n",
    "\n",
    "    # Reset random seed (so it doesn't affect global)\n",
    "    # np.random.seed(None)\n",
    "\n",
    "    return distorted\n",
    "\n",
    "def _signal_distort_artifacts(\n",
    "    signal,\n",
    "    signal_sd=None,\n",
    "    sampling_rate=1000,\n",
    "    artifacts_frequency=0,\n",
    "    artifacts_amplitude=0.1,\n",
    "    artifacts_number=5,\n",
    "    artifacts_shape=\"laplace\",\n",
    "    silent=False,\n",
    "):\n",
    "\n",
    "    # Generate artifact burst with random onset and random duration.\n",
    "    artifacts = _signal_distort_noise(\n",
    "        len(signal),\n",
    "        sampling_rate=sampling_rate,\n",
    "        noise_frequency=artifacts_frequency,\n",
    "        noise_amplitude=artifacts_amplitude,\n",
    "        noise_shape=artifacts_shape,\n",
    "        silent=silent,\n",
    "    )\n",
    "    if artifacts.sum() == 0:\n",
    "        return artifacts\n",
    "\n",
    "    min_duration = int(np.rint(len(artifacts) * 0.001))\n",
    "    max_duration = int(np.rint(len(artifacts) * 0.01))\n",
    "    artifact_durations = np.random.randint(min_duration, max_duration, artifacts_number)\n",
    "\n",
    "    artifact_onsets = np.random.randint(0, len(artifacts) - max_duration, artifacts_number)\n",
    "    artifact_offsets = artifact_onsets + artifact_durations\n",
    "\n",
    "    artifact_idcs = np.array([False] * len(artifacts))\n",
    "    for i in range(artifacts_number):\n",
    "        artifact_idcs[artifact_onsets[i] : artifact_offsets[i]] = True\n",
    "\n",
    "    artifacts[~artifact_idcs] = 0\n",
    "\n",
    "    # Scale amplitude by the signal's standard deviation.\n",
    "    if signal_sd is not None:\n",
    "        artifacts_amplitude *= signal_sd\n",
    "    artifacts *= artifacts_amplitude\n",
    "\n",
    "    return artifacts\n",
    "\n",
    "def _signal_distort_noise_multifrequency(\n",
    "    signal,\n",
    "    signal_sd=None,\n",
    "    sampling_rate=1000,\n",
    "    noise_amplitude=0.1,\n",
    "    noise_frequency=100,\n",
    "    noise_shape=\"laplace\",\n",
    "    silent=False,\n",
    "):\n",
    "    base_noise = np.zeros(len(signal))\n",
    "    params = listify(\n",
    "        noise_amplitude=noise_amplitude, noise_frequency=noise_frequency, noise_shape=noise_shape\n",
    "    )\n",
    "\n",
    "    for i in range(len(params[\"noise_amplitude\"])):\n",
    "\n",
    "        freq = params[\"noise_frequency\"][i]\n",
    "        amp = params[\"noise_amplitude\"][i]\n",
    "        shape = params[\"noise_shape\"][i]\n",
    "\n",
    "        if signal_sd is not None:\n",
    "            amp *= signal_sd\n",
    "\n",
    "        # Make some noise!\n",
    "        _base_noise = _signal_distort_noise(\n",
    "            len(signal),\n",
    "            sampling_rate=sampling_rate,\n",
    "            noise_frequency=freq,\n",
    "            noise_amplitude=amp,\n",
    "            noise_shape=shape,\n",
    "            silent=silent,\n",
    "        )\n",
    "        base_noise += _base_noise\n",
    "\n",
    "    return base_noise\n",
    "\n",
    "\n",
    "def _signal_distort_noise(\n",
    "    n_samples,\n",
    "    sampling_rate=1000,\n",
    "    noise_frequency=100,\n",
    "    noise_amplitude=0.1,\n",
    "    noise_shape=\"laplace\",\n",
    "    silent=False,\n",
    "):\n",
    "\n",
    "    _noise = np.zeros(n_samples)\n",
    "    # Apply a very conservative Nyquist criterion in order to ensure\n",
    "    # sufficiently sampled signals.\n",
    "    nyquist = sampling_rate * 0.1\n",
    "    if noise_frequency > nyquist:\n",
    "        if not silent:\n",
    "            warn(\n",
    "                f\"Skipping requested noise frequency \"\n",
    "                f\" of {noise_frequency} Hz since it cannot be resolved at \"\n",
    "                f\" the sampling rate of {sampling_rate} Hz. Please increase \"\n",
    "                f\" sampling rate to {noise_frequency * 10} Hz or choose \"\n",
    "                f\" frequencies smaller than or equal to {nyquist} Hz.\",\n",
    "                category=NeuroKitWarning,\n",
    "            )\n",
    "        return _noise\n",
    "    # Also make sure that at least one period of the frequency can be\n",
    "    # captured over the duration of the signal.\n",
    "    duration = n_samples / sampling_rate\n",
    "    if (1 / noise_frequency) > duration:\n",
    "        if not silent:\n",
    "            warn(\n",
    "                f\"Skipping requested noise frequency \"\n",
    "                f\" of {noise_frequency} Hz since its period of {1 / noise_frequency} \"\n",
    "                f\" seconds exceeds the signal duration of {duration} seconds. \"\n",
    "                f\" Please choose noise frequencies larger than \"\n",
    "                f\" {1 / duration} Hz or increase the duration of the \"\n",
    "                f\" signal above {1 / noise_frequency} seconds.\",\n",
    "                category=NeuroKitWarning,\n",
    "            )\n",
    "        return _noise\n",
    "\n",
    "    noise_duration = int(duration * noise_frequency)\n",
    "\n",
    "    if noise_shape in [\"normal\", \"gaussian\"]:\n",
    "        _noise = np.random.normal(0, noise_amplitude, noise_duration)\n",
    "    elif noise_shape == \"laplace\":\n",
    "        _noise = np.random.laplace(0, noise_amplitude, noise_duration)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"NeuroKit error: signal_distort(): 'noise_shape' should be one of 'gaussian' or 'laplace'.\"\n",
    "        )\n",
    "\n",
    "    if len(_noise) != n_samples:\n",
    "        _noise = signal_resample(_noise, desired_length=n_samples, method=\"interpolation\")\n",
    "    return _noise\n",
    "\n",
    "\n",
    "def _signal_distort_powerline(\n",
    "    signal,\n",
    "    signal_sd=None,\n",
    "    sampling_rate=1000,\n",
    "    powerline_frequency=50,\n",
    "    powerline_amplitude=0.1,\n",
    "    silent=False,\n",
    "):\n",
    "\n",
    "    duration = len(signal) / sampling_rate\n",
    "    powerline_noise = signal_simulate(\n",
    "        duration=duration,\n",
    "        sampling_rate=sampling_rate,\n",
    "        frequency=powerline_frequency,\n",
    "        amplitude=1,\n",
    "        silent=silent,\n",
    "    )\n",
    "\n",
    "    if signal_sd is not None:\n",
    "        powerline_amplitude *= signal_sd\n",
    "    powerline_noise *= powerline_amplitude\n",
    "\n",
    "    return powerline_noise\n",
    "import scipy\n",
    "import scipy.io as sio\n",
    "from scipy.signal import butter, filtfilt, lfilter\n",
    "from scipy.signal import kaiserord, firwin, filtfilt, butter\n",
    "\n",
    "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n",
    "    Arguments:\n",
    "        indices: a list of indices\n",
    "        num_samples: number of samples to draw\n",
    "        callback_get_label: a callback-like function which takes two arguments - dataset and index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, indices = None, num_samples = None, callback_get_label = None):\n",
    "        self.indices = list(range(len(dataset))) if indices is None else indices        # if indices is not provided, all elements in the dataset will be considered\n",
    "        self.callback_get_label = callback_get_label                                    # define custom callback\n",
    "        self.num_samples = len(self.indices) if num_samples is None else num_samples    # if num_samples is not provided, draw `len(indices)` samples in each iteration\n",
    "\n",
    "        df = pd.DataFrame()                                                             # distribution of classes in the dataset\n",
    "        \n",
    "        label = []\n",
    "        for idx in trange(len(dataset), desc=\"Sampling\"):\n",
    "            ########## customize here ###############\n",
    "            l = dataset[idx]['y_PVC_seg']\n",
    "            if 1 in l:\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(0)                \n",
    "            ########## customize here ###############\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        df[\"label\"] = label\n",
    "        df.index = self.indices\n",
    "        df = df.sort_index()\n",
    "\n",
    "        label_to_count = df[\"label\"].value_counts()\n",
    "\n",
    "        weights = 1.0 / label_to_count[df[\"label\"]] # almost equally\n",
    "        # weights = 1.0 / (label_to_count[df[\"label\"]])**2 # slightly weighted to 1\n",
    "        self.weights = torch.DoubleTensor(weights.to_list())\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def IIRRemoveBL(ecgy,Fs, Fc):    \n",
    "    #    ecgy:        the contamined signal (must be a list)\n",
    "    #    Fc:          cut-off frequency\n",
    "    #    Fs:          sample frequiency\n",
    "    #    ECG_Clean :  processed signal without BLW\n",
    "    \n",
    "    # getting the length of the signal\n",
    "    signal_len = len(ecgy)\n",
    "    \n",
    "    # fixed order\n",
    "    N = 4\n",
    "    \n",
    "    # Normalized Cutt of frequency\n",
    "    Wn = Fc/(Fs/2)    \n",
    "    \n",
    "    # IIR butterworth coefficients\n",
    "    b, a = butter(N, Wn, 'highpass', analog=False)\n",
    "    \n",
    "    # Check filtfilt condition\n",
    "    if N*3 > signal_len:\n",
    "        diff = N*3 - signal_len\n",
    "        ecgy = list(reversed(ecgy)) + list(ecgy) + list(ecgy[-1] * np.ones(diff))\n",
    "        \n",
    "        # Filtering with filtfilt\n",
    "        ECG_Clean = filtfilt(b, a, ecgy)\n",
    "        ECG_Clean = ECG_Clean[signal_len: signal_len + signal_len]\n",
    "        \n",
    "    else:\n",
    "        ECG_Clean = filtfilt(b, a, ecgy)\n",
    "                   \n",
    "    return ECG_Clean\n",
    "\n",
    "def IIRRemoveHF(ecgy, Fs, Fc):\n",
    "    #    ecgy:        the contamined signal (must be a list)\n",
    "    #    Fc:          cut-off frequency\n",
    "    #    Fs:          sample frequiency\n",
    "    #    ECG_Clean :  processed signal without BLW\n",
    "\n",
    "    # getting the length of the signal\n",
    "    signal_len = len(ecgy)\n",
    "\n",
    "    # fixed order\n",
    "    N = 4\n",
    "\n",
    "    # Normalized Cutt of frequency\n",
    "    Wn = Fc / (Fs / 2)\n",
    "\n",
    "    # IIR butterworth coefficients\n",
    "    b, a = butter(N, Wn, 'lowpass', analog=False)\n",
    "\n",
    "    # Check filtfilt condition\n",
    "    if N * 3 > signal_len:\n",
    "        diff = N * 3 - signal_len\n",
    "        ecgy = list(reversed(ecgy)) + list(ecgy) + list(ecgy[-1] * np.ones(diff))\n",
    "\n",
    "        # Filtering with filtfilt\n",
    "        ECG_Clean = filtfilt(b, a, ecgy)\n",
    "        ECG_Clean = ECG_Clean[signal_len: signal_len + signal_len]\n",
    "\n",
    "    else:\n",
    "        ECG_Clean = filtfilt(b, a, ecgy)\n",
    "\n",
    "    return ECG_Clean\n",
    "\n",
    "def remove_baseline_wander(signal, fs):    \n",
    "    Fc_l = 0.5\n",
    "    Fc_h = 40.0\n",
    "\n",
    "    signal_IIR = IIRRemoveBL(signal,fs,Fc_l)\n",
    "    signal_IIR = IIRRemoveHF(signal_IIR,fs,Fc_h)\n",
    "    return signal_IIR\n",
    "\n",
    "# old preprocessing\n",
    "def remove_baseline_wander(signal, fs):    \n",
    "    order = 4\n",
    "    nyq = 0.5 * fs\n",
    "    lowcut = 0.67 #0.5\n",
    "    highcut = 40\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    \n",
    "    res = filtfilt(b, a, signal)\n",
    "    # res = lfilter(b, a, signal)\n",
    "    return res\n",
    "\n",
    "from audiomentations import *\n",
    "p=.2\n",
    "augment_audiomentation = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.01, p=p),\n",
    "    AddGaussianSNR(min_snr_in_db=5, max_snr_in_db=40.0, p=p),\n",
    "    Gain(min_gain_in_db=-12, max_gain_in_db=12, p=p),\n",
    "    FrequencyMask(min_frequency_band=0.0, max_frequency_band=.5, p=p),\n",
    "    TanhDistortion(min_distortion= 0.01, max_distortion = 0.4, p=p),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=p),\n",
    "])\n",
    "\n",
    "import neurokit2 as nk\n",
    "def augment_neurokit(ecg_signal, sr):\n",
    "    noise_shape = ['gaussian', 'laplace']\n",
    "    n_noise_shape = np.random.randint(0,2)\n",
    "\n",
    "    powerline_frequency = np.random.randint(50,60)\n",
    "    noise_frequency = np.random.randint(2,20)\n",
    "    artifacts_frequency= np.random.randint(2,20)\n",
    "    # artifacts_number = np.random.randint(2,20)\n",
    "    artifacts_number = 1\n",
    "\n",
    "    powerline_amplitude = np.random.rand(1)*.3 #/ powerline_frequency\n",
    "    noise_amplitude = np.random.rand(1)*.2 #/ noise_frequency\n",
    "    artifacts_amplitude = np.random.rand(1)*1 #/ artifacts_frequency\n",
    "    \n",
    "    ecg_signal = signal_distort(ecg_signal,\n",
    "                                sampling_rate=sr,\n",
    "                                noise_shape=noise_shape[n_noise_shape],\n",
    "                                noise_amplitude=noise_amplitude,\n",
    "                                noise_frequency=noise_frequency,\n",
    "                                powerline_amplitude=powerline_amplitude,\n",
    "                                powerline_frequency=powerline_frequency,\n",
    "                                artifacts_amplitude=artifacts_amplitude,\n",
    "                                artifacts_frequency=artifacts_frequency,\n",
    "                                artifacts_number=artifacts_number,\n",
    "                                linear_drift=False,\n",
    "                                random_state=None,#42,\n",
    "                                silent=True)\n",
    "    return ecg_signal\n",
    "\n",
    "def minmax(arr):\n",
    "    \"\"\"\n",
    "    numpy\n",
    "    \"\"\"\n",
    "    return (arr-np.min(arr))/(np.max(arr)-np.min(arr))\n",
    "\n",
    "def augment_neurokit2(sig,sr):\n",
    "    beta = (np.random.rand(1)-.5)*4\n",
    "    amp = np.random.rand(1)\n",
    "    \n",
    "    noise = nk.signal.signal_noise(duration=len(sig)/sr, sampling_rate=sr, beta=beta)*amp\n",
    "    noise = minmax(noise) * (np.random.rand(1)) / 4\n",
    "\n",
    "    result = augment_neurokit(noise, sr=sr)\n",
    "    result = sig + result\n",
    "    result = minmax(result)\n",
    "    return result\n",
    "\n",
    "class MIT_DATASET():\n",
    "    def __init__(self, data, featureLength, srTarget, classes=4, augmentation=\"NONE\", random_crop=False):\n",
    "        self.data = data\n",
    "        self.classes = classes\n",
    "        self.augmentation = augmentation\n",
    "        if augmentation == \"NONE\":\n",
    "            self.augmentation = False\n",
    "        elif augmentation =='NEUROKIT':\n",
    "            self.augmentation=augment_neurokit\n",
    "        elif augmentation =='NEUROKIT2':\n",
    "            self.augmentation=augment_neurokit2\n",
    "        elif augmentation =='AUDIOMENTATION':\n",
    "            self.augmentation=augment_audiomentation\n",
    "\n",
    "        self.random_crop = random_crop\n",
    "        self.srTarget = srTarget\n",
    "        self.featureLength = featureLength\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pid = self.data[idx]['pid']\n",
    "        signal = self.data[idx]['signal']\n",
    "        srOriginal = self.data[idx]['sr']\n",
    "        time = self.data[idx]['time']\n",
    "        idx_Normal = self.data[idx]['idx_Normal']\n",
    "        idx_PVC = self.data[idx]['idx_PVC']\n",
    "        idx_AFIB = self.data[idx]['idx_Afib']\n",
    "        idx_Others = self.data[idx]['idx_Others']\n",
    "        idx_Artifact = self.data[idx]['idx_Artifact']\n",
    "        dataSource = self.data[idx]['dataSource']\n",
    "        \n",
    "        y_Normal_seg = np.zeros_like(signal)\n",
    "        y_PVC_seg = np.zeros_like(signal)\n",
    "        y_AFIB_seg = np.zeros_like(signal)\n",
    "        y_Others_seg = np.zeros_like(signal)\n",
    "        \n",
    "        interval = int(srOriginal * 0.1) # this is to set peak interval\n",
    "        \n",
    "        # grab annotations\n",
    "        for idx_ in idx_Normal:  \n",
    "            y_Normal_seg[idx_-interval:idx_+interval] = 1\n",
    "        for idx_ in idx_PVC:\n",
    "            y_PVC_seg[idx_-interval:idx_+interval] = 1\n",
    "        for idx_ in idx_AFIB:\n",
    "            y_AFIB_seg[idx_-interval:idx_+interval] = 1\n",
    "        for idx_ in idx_Others:\n",
    "            y_Others_seg[idx_-interval:idx_+interval] = 1\n",
    "    \n",
    "        # resampling\n",
    "        if self.augmentation:\n",
    "            srTarget = np.random.randint(int(self.srTarget*0.97),int(self.srTarget*1.03)) # time stretching, you need to carefully check here\n",
    "        else: \n",
    "            srTarget = self.srTarget\n",
    "            \n",
    "        signal = lb.resample(signal, orig_sr=srOriginal, target_sr=srTarget) if srTarget != srOriginal else signal # resample\n",
    "        y_Normal_seg = scipy.ndimage.zoom(y_Normal_seg, srTarget/srOriginal, order=0, mode='nearest',) if srTarget != srOriginal else y_Normal_seg # resample\n",
    "        y_PVC_seg = scipy.ndimage.zoom(y_PVC_seg, srTarget/srOriginal, order=0, mode='nearest',) if srTarget != srOriginal else y_PVC_seg # resample\n",
    "        y_AFIB_seg = scipy.ndimage.zoom(y_AFIB_seg, srTarget/srOriginal, order=0, mode='nearest',) if srTarget != srOriginal else y_AFIB_seg # resample\n",
    "        y_Others_seg = scipy.ndimage.zoom(y_Others_seg, srTarget/srOriginal, order=0, mode='nearest',) if srTarget != srOriginal else y_Others_seg # resample\n",
    "        \n",
    "        if self.random_crop:\n",
    "            if int(len(signal)) > self.featureLength:  # randomly crop \n",
    "                randnum = np.random.randint(0,len(signal)-self.featureLength)\n",
    "                start = randnum if self.random_crop else 0\n",
    "                end = start+self.featureLength\n",
    "            elif int(len(signal)) == self.featureLength:\n",
    "                start = 0\n",
    "                end = 0 + self.featureLength\n",
    "            else:\n",
    "                print('too short data:: need check sampling rate or featureLength', int(len(signal)),self.featureLength)\n",
    "                \n",
    "            signal = signal[start:end]\n",
    "            y_Normal_seg = y_Normal_seg[start:end]\n",
    "            y_PVC_seg = y_PVC_seg[start:end]\n",
    "            y_AFIB_seg = y_AFIB_seg[start:end]\n",
    "            y_Others_seg = y_Others_seg[start:end]\n",
    "        # print('after crop',signal.shape)\n",
    "\n",
    "        y_peak_seg = y_Normal_seg + y_PVC_seg + y_Others_seg + y_AFIB_seg # R-peak\n",
    "        y_peak_seg[y_peak_seg!=0] =1\n",
    "\n",
    "        if self.classes == 1:\n",
    "            y_seg = np.expand_dims(y_PVC_seg,0) # 1 class\n",
    "        elif self.classes == 2:\n",
    "            y_seg = np.stack((y_peak_seg, y_PVC_seg), axis=0).astype(float) # 2 multi class\n",
    "        elif self.classes == 3:\n",
    "            y_Others_seg = y_Others_seg + y_AFIB_seg # non PVC\n",
    "            y_Others_seg[y_Others_seg!=0] =1\n",
    "            y_seg = np.stack((y_peak_seg, y_PVC_seg, y_Others_seg), axis=0).astype(float) # 3 multi class    \n",
    "            # y_seg = np.stack((y_peak_seg, y_PVC_seg, y_AFIB_seg), axis=0).astype(float) # 3 multi class    \n",
    "        elif self.classes == 4:\n",
    "            y_seg = np.stack((y_peak_seg, y_PVC_seg, y_Others_seg, y_AFIB_seg), axis=0).astype(float) # 4 multi class\n",
    "        \n",
    "        y_Normal = np.array([0]) if 1 in y_Normal_seg else np.array([1]) # classification task\n",
    "        y_Others = np.array([0]) if 1 in y_Others_seg else np.array([1]) # classification task\n",
    "        y_PVC    = np.array([0]) if 1 in y_PVC_seg else np.array([1]) # classification task\n",
    "        y_AFIB   = np.array([0]) if 1 in y_AFIB_seg else np.array([1]) # classification task\n",
    "        \n",
    "        signal_original = signal.copy()\n",
    "        signal_original = np.expand_dims(signal_original,0)\n",
    "        \n",
    "        # augmentation\n",
    "        signal = signal if not self.augmentation else self.augmentation(signal, srTarget)\n",
    "        \n",
    "        signal = remove_baseline_wander(signal,srTarget)\n",
    "        signal = np.expand_dims(signal,0)\n",
    "                      \n",
    "        signal = (signal - np.min(signal)) / (np.max(signal) - np.min(signal)) # normalize  \n",
    "        signal = torch.tensor(signal).float() # shape should be Channel X Signal\n",
    "        \n",
    "        return {'dataSource':dataSource,\n",
    "                'pid':pid,\n",
    "                'srOriginal': srOriginal,\n",
    "                'srTarget':srTarget,\n",
    "                'time':time,\n",
    "                'fname':f'{pid}_time{time}',\n",
    "                'signal':signal,\n",
    "                'signal_original':signal_original,\n",
    "                'y_AFIB':y_AFIB, \n",
    "                'y_PVC':y_PVC,\n",
    "                'y_AFIB_seg':y_AFIB_seg,\n",
    "                'y_PVC_seg':y_PVC_seg, \n",
    "                'y_Normal_seg':y_Normal_seg,\n",
    "                'y_Others_seg':y_Others_seg, \n",
    "                'y_seg':y_seg,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ddae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=.1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma \n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #first compute binary cross-entropy \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Pt = torch.exp(-BCE)\n",
    "        alpha_tensor = (1 - self.alpha) + targets * (2 * self.alpha - 1)  # alpha if target = 1 and 1 - alpha if target = 0\n",
    "        focal_loss = self.alpha * (1-Pt)**self.gamma * BCE\n",
    "                       \n",
    "        return focal_loss\n",
    "\n",
    "# class WeightedFocalLoss(nn.Module):\n",
    "#     \"Non weighted version of Focal Loss\"\n",
    "#     def __init__(self, alpha=.1, gamma=2):\n",
    "#         super(WeightedFocalLoss, self).__init__()\n",
    "#         self.alpha = torch.tensor([alpha, 1-alpha])\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "#         BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "#         targets = targets.type(torch.long)\n",
    "#         at = self.alpha.gather(0, targets.data.view(-1))\n",
    "#         pt = torch.exp(-BCE_loss)\n",
    "#         F_loss = at*(1-pt)**self.gamma * BCE_loss\n",
    "#         return F_loss.mean()\n",
    "    \n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=.25, gamma=2, weight=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma \n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #first compute binary cross-entropy \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        BCE_EXP = torch.exp(-BCE)\n",
    "        focal_loss = self.alpha * (1-BCE_EXP)**self.gamma * BCE\n",
    "                       \n",
    "        return focal_loss\n",
    "    \n",
    "def get_dice_loss(y_pred, y_true, log=False, per_image=False, smooth=1e-7):\n",
    "    tp = torch.sum(y_true * y_pred, axis=AXIS)\n",
    "    fp = torch.sum(y_pred, axis=AXIS) - tp\n",
    "    fn = torch.sum(y_true, axis=AXIS) - tp\n",
    "    dice_score_per_image = (2 * tp + smooth) / (2 * tp + fp + fn + smooth)\n",
    "    if log:\n",
    "        dice_score_per_image = -1 * torch.log(dice_score_per_image)\n",
    "    else:\n",
    "        dice_score_per_image = 1 - dice_score_per_image\n",
    "    if per_image:\n",
    "        return dice_score_per_image\n",
    "    else:\n",
    "        return torch.mean(dice_score_per_image)\n",
    "    \n",
    "def get_tversky_loss(y_pred, y_true, beta=0.7, log=False, per_image=False, smooth=1e-7):\n",
    "    alpha = 1 - beta\n",
    "    tp = torch.sum(y_true * y_pred, axis=AXIS)\n",
    "    fp = torch.sum(y_pred, axis=AXIS) - tp\n",
    "    fn = torch.sum(y_true, axis=AXIS) - tp\n",
    "    dice_score_per_image = (tp + smooth) / (tp + alpha * fp + beta * fn + smooth)\n",
    "    if log:\n",
    "        dice_score_per_image = -1 * torch.log(dice_score_per_image)\n",
    "    else:\n",
    "        dice_score_per_image = 1 - dice_score_per_image\n",
    "    if per_image:\n",
    "        return dice_score_per_image\n",
    "    else:\n",
    "        return torch.mean(dice_score_per_image)\n",
    "\n",
    "class PropotionalLoss(nn.Module):\n",
    "    def __init__(self, log=False, per_image=False, smooth=1e-7, beta=0.7, bce=False):\n",
    "        super(PropotionalLoss, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.smooth = smooth \n",
    "        self.log = log\n",
    "        self.per_image = per_image\n",
    "        self.bce = bce\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        AXIS = [-1]\n",
    "        self.alpha = 1 - self.beta\n",
    "        y_true = targets\n",
    "        y_pred = inputs\n",
    "        \n",
    "        prevalence = torch.mean(y_true, axis=AXIS)\n",
    "        tp = torch.sum(y_true * y_pred, axis=AXIS)\n",
    "        tn = torch.sum((1 - y_true) * (1 - y_pred), axis=AXIS)\n",
    "        fp = torch.sum(y_pred, axis=AXIS) - tp\n",
    "        fn = torch.sum(y_true, axis=AXIS) - tp\n",
    "        negative_score = (tn + self.smooth) / (tn + self.beta * fn + self.alpha * fp + self.smooth) * (self.smooth + 1 - prevalence)\n",
    "        positive_score = (tp + self.smooth) / (tp + self.alpha * fn + self.beta * fp + self.smooth) * (self.smooth + prevalence)\n",
    "        score_per_image = negative_score + positive_score\n",
    "        \n",
    "        if self.log:\n",
    "            score_per_image = -1 * torch.log(score_per_image)\n",
    "        else:\n",
    "            score_per_image = 1 - score_per_image\n",
    "            \n",
    "        if self.per_image == False:\n",
    "            score_per_image = torch.mean(score_per_image)\n",
    "        \n",
    "        if self.bce:\n",
    "            return score_per_image + F.binary_cross_entropy(y_pred, y_true)       \n",
    "        \n",
    "        \n",
    "# def get_propotional_loss(y_pred, y_true, log=False, per_image=False, smooth=SMOOTH, beta=0.7):\n",
    "#     alpha = 1 - beta\n",
    "#     prevalence = torch.mean(y_true, axis=AXIS)\n",
    "#     tp = torch.sum(y_true * y_pred, axis=AXIS)\n",
    "#     tn = torch.sum((1 - y_true) * (1 - y_pred), axis=AXIS)\n",
    "#     fp = torch.sum(y_pred, axis=AXIS) - tp\n",
    "#     fn = torch.sum(y_true, axis=AXIS) - tp\n",
    "#     negative_score = (tn + smooth) \\\n",
    "#         / (tn + beta * fn + alpha * fp + smooth) * (smooth + 1 - prevalence)\n",
    "#     positive_score = (tp + smooth) \\\n",
    "#         / (tp + alpha * fn + beta * fp + smooth) * (smooth + prevalence)\n",
    "#     score_per_image = negative_score + positive_score\n",
    "#     if log:\n",
    "#         score_per_image = -1 * torch.log(score_per_image)\n",
    "#     else:\n",
    "#         score_per_image = 1 - score_per_image\n",
    "#     if per_image:\n",
    "#         return score_per_image\n",
    "#     else:\n",
    "#         return torch.mean(score_per_image)\n",
    "    \n",
    "# get_bce_loss = torch.nn.BCELoss()\n",
    "# get_loss = lambda y_pred, y_true: get_propotional_loss(y_pred, y_true) + get_bce_loss(y_pred, y_true)\n",
    "    \n",
    "# def BCELoss_class_weighted(weights):\n",
    "\n",
    "#     def loss(input, target):\n",
    "#         # input = torch.clamp(input, min=1e-7, max=1-1e-7)\n",
    "#         bce = - weights[1] * target * torch.log(input) - weights[0] * (1 - target) * torch.log(1 - input)\n",
    "#         return torch.mean(bce)\n",
    "\n",
    "#     return loss\n",
    "\n",
    "# from libauc.losses import APLoss\n",
    "# from libauc.optimizers import SOAP\n",
    "# from libauc.models import resnet18 as ResNet18\n",
    "# from libauc.datasets import CIFAR10\n",
    "# from libauc.utils import ImbalancedDataGenerator\n",
    "# from libauc.sampler import DualSampler\n",
    "# from libauc.metrics import auc_prc_score\n",
    "\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import Dataset\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "\n",
    "# model = ResNet18(pretrained=False, last_activation=None) \n",
    "# model = model.cuda()\n",
    "\n",
    "# lr= 1e-3\n",
    "# pos_len=6018\n",
    "# margin = 1.0\n",
    "# gamma = 0.1\n",
    "\n",
    "# weight_decay = 0\n",
    "# total_epoch = 60\n",
    "# decay_epoch = [30]\n",
    "# SEED = 2022\n",
    "\n",
    "# lossFn = APLoss(pos_len=pos_len, margin=margin, gamma=gamma)\n",
    "# optimizer = SOAP(model.parameters(), lr=lr, mode='adam', weight_decay=weight_decay)\n",
    "\n",
    "# lossFn = FocalLoss(alpha=1, gamma=0)\n",
    "# y = torch.zeros(1,4,64)\n",
    "# y[:,:1,:]=1\n",
    "\n",
    "# print(np.unique(y.numpy(),return_counts=True))\n",
    "# yhat = torch.rand(1,4,64)\n",
    "# print(lossFn(yhat,y))\n",
    "\n",
    "# lossFn = nn.BCELoss()\n",
    "# print(lossFn(yhat,y))\n",
    "\n",
    "# lossFn = BCELoss_class_weighted([.2, 1])\n",
    "# print(lossFn(yhat,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a189021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, StochasticWeightAveraging, LambdaCallback, EarlyStopping\n",
    "\n",
    "def train():\n",
    "    set_seed()\n",
    "    wandb.init(config=config_defaults)\n",
    "    \n",
    "    hyperparameters = dict(wandb.config)\n",
    "    model = PVC_NET(hyperparameters)\n",
    "    # model.prepare_data()\n",
    "    # model.train_dataloader()\n",
    "\n",
    "    classes = model.hyperparameters['outChannels']\n",
    "    srTarget = model.hyperparameters['srTarget']\n",
    "    featureLength = model.hyperparameters['featureLength']   \n",
    "#     classes = wandb.config.outChannels\n",
    "#     srTarget = wandb.config.srTarget\n",
    "#     featureLength = wandb.config.featureLength\n",
    "\n",
    "    files = glob('dataset/MIT-BIH_NPY/*.npy')\n",
    "    \n",
    "    def seed_MITBIH(files, seed):\n",
    "        train_files, valid_files = sklearn.model_selection.train_test_split(files, test_size=.2, random_state= seed)\n",
    "\n",
    "        train_seg = []\n",
    "        for f in train_files:\n",
    "            data = np.load(f,allow_pickle=True)\n",
    "            train_seg.extend(data)\n",
    "\n",
    "        valid_seg = []\n",
    "        for f in valid_files:\n",
    "            data = np.load(f,allow_pickle=True)\n",
    "            valid_seg.extend(data)\n",
    "            \n",
    "        print('seed:',seed)\n",
    "        # print('train_files:',train_files)\n",
    "        # print('valid_files:',valid_files)\n",
    "        add_datainfo(train_seg,1)\n",
    "        add_datainfo(valid_seg,2)\n",
    "        return train_seg, valid_seg\n",
    "    \n",
    "    train_data, valid_data = seed_MITBIH(files, wandb.config.dataSeed)\n",
    "        \n",
    "    train_dataset = MIT_DATASET(train_data,featureLength,srTarget,classes,wandb.config.trainaug, True)\n",
    "    valid_dataset = MIT_DATASET(valid_data,featureLength,srTarget,classes,False)\n",
    "    test_dataset = MIT_DATASET(test_data,featureLength,srTarget,classes,False)\n",
    "    AMC_dataset = MIT_DATASET(AMC_data,featureLength,srTarget,classes,False)\n",
    "    INCART_dataset = MIT_DATASET(INCART_data,featureLength, srTarget, classes, False)\n",
    "    Fantasia_dataset = MIT_DATASET(Fantasia_data,featureLength, srTarget, classes, False)\n",
    "    CPSC2020_dataset = MIT_DATASET(CPSC2020_data,featureLength, srTarget, classes, False)\n",
    "\n",
    "    if wandb.config.sampler:\n",
    "        train_loader = DataLoader(train_dataset, batch_size = wandb.config.batch_size, shuffle = False, num_workers=4, pin_memory=True, sampler=ImbalancedDatasetSampler(train_dataset))\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size = wandb.config.batch_size, shuffle = True, num_workers=4, pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size = 64, shuffle = False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=2, shuffle = False)\n",
    "    AMC_loader = DataLoader(AMC_dataset,batch_size = 64, num_workers=2, shuffle = False)\n",
    "    INCART_loader = DataLoader(INCART_dataset, batch_size = 64, num_workers=2, shuffle = False)\n",
    "    Fantasia_loader = DataLoader(Fantasia_dataset,batch_size = 64, num_workers=2, shuffle = False)\n",
    "    CPSC2020_loader = DataLoader(CPSC2020_dataset,batch_size = 64, num_workers=2, shuffle = False)\n",
    "\n",
    "    wandb_logger = pl_loggers.WandbLogger(save_dir=f\"{wandb.config.path_logRoot}/{model.experiment_name}\", name=model.experiment_name, project=wandb.config.project, offline=False)\n",
    "\n",
    "    lr_monitor_callback = LearningRateMonitor(logging_interval='epoch',)\n",
    "    early_stop_callback = EarlyStopping(monitor='val_loss', mode=\"min\", patience=10, verbose=False)\n",
    "    loss_checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', dirpath=f\"{wandb.config.path_logRoot}/{model.experiment_name}/weight/\", filename=\"best_val_loss\", save_top_k=1, verbose=False)\n",
    "    # metric_checkpoint_callback = ModelCheckpoint(monitor='val_AUPRC_Class1Raw', mode='max', dirpath=f\"{wandb.config.path_logRoot}/{model.experiment_name}/weight/\", filename=\"best_val_metric\", save_top_k=1, verbose=False)\n",
    "\n",
    "    trainer = pl.Trainer(accumulate_grad_batches=8,\n",
    "                        gradient_clip_val=0.1,\n",
    "                        accelerator='gpu',\n",
    "                        devices=-1,\n",
    "                        strategy ='dp',\n",
    "                        max_epochs=100, # 80\n",
    "                        sync_batchnorm=True,\n",
    "                        benchmark=False,\n",
    "                        deterministic=True,\n",
    "                        check_val_every_n_epoch=1,\n",
    "                        # callbacks=[loss_checkpoint_callback, metric_checkpoint_callback, lr_monitor_callback, early_stop_callback],# StochasticWeightAveraging(swa_lrs=0.05)], #\n",
    "                        callbacks=[loss_checkpoint_callback, lr_monitor_callback, early_stop_callback],# , StochasticWeightAveraging(swa_lrs=0.0001)], #\n",
    "                        logger = wandb_logger,\n",
    "                        precision= 32 # 'bf16', 16, 32\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_loader, valid_loader)\n",
    "    \n",
    "    # weights = glob(f\"{hyperparameters['path_logRoot']}/{net.experiment_name}/weight/*val_loss*\")[-1]\n",
    "    # result_test = trainer.test(net, test_loader, ckpt_path=weights)\n",
    "    # result_AMC = trainer.test(net, AMC_loader, ckpt_path=weights)\n",
    "    # result_INCART = trainer.test(net, INCART_loader,ckpt_path=weights)\n",
    "    # result_CPSC2020 = trainer.test(net, CPSC2020_loader,ckpt_path=weights)\n",
    "\n",
    "    result_test = trainer.test(model, test_loader, ckpt_path='best')\n",
    "    result_AMC = trainer.test(model, AMC_loader, ckpt_path='best')\n",
    "    result_CPSC2020 = trainer.test(model, CPSC2020_loader,ckpt_path='best')\n",
    "    result_INCART = trainer.test(model, INCART_loader,ckpt_path='best')\n",
    "    \n",
    "\n",
    "def test(path):\n",
    "    # set_seed()\n",
    "    model = PVC_NET.load_from_checkpoint(path)\n",
    "\n",
    "    classes = model.hyperparameters['outChannels']\n",
    "    srTarget = model.hyperparameters['srTarget']\n",
    "    featureLength = model.hyperparameters['featureLength']\n",
    "    \n",
    "    test_dataset = MIT_DATASET(test_data,featureLength,srTarget,classes,False)\n",
    "    AMC_dataset = MIT_DATASET(AMC_data,featureLength,srTarget,classes,False)\n",
    "    INCART_dataset = MIT_DATASET(INCART_data,featureLength, srTarget, classes, False)\n",
    "    Fantasia_dataset = MIT_DATASET(Fantasia_data,featureLength, srTarget, classes, False)\n",
    "    CPSC2020_dataset = MIT_DATASET(CPSC2020_data,featureLength, srTarget, classes, False)\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size = 64, num_workers=2, shuffle = False)\n",
    "    AMC_loader = DataLoader(AMC_dataset,batch_size = 64, num_workers=2, shuffle = False)\n",
    "    INCART_loader = DataLoader(INCART_dataset, batch_size = 64, num_workers=2, shuffle = False)\n",
    "    Fantasia_loader = DataLoader(Fantasia_dataset,batch_size = 64, num_workers=2, shuffle = False)\n",
    "    CPSC2020_loader = DataLoader(CPSC2020_dataset,batch_size = 64, num_workers=2, shuffle = False)\n",
    "\n",
    "    trainer = pl.Trainer(accumulate_grad_batches=8,\n",
    "                        gradient_clip_val=0.1,\n",
    "                        accelerator='gpu',\n",
    "                        devices=-1,\n",
    "                        strategy ='dp',\n",
    "                        max_epochs=100, # 80\n",
    "                        sync_batchnorm=True,\n",
    "                        benchmark=False,\n",
    "                        deterministic=True,\n",
    "                        check_val_every_n_epoch=1,\n",
    "                        # callbacks=[loss_checkpoint_callback, lr_monitor_callback, early_stop_callback],# , StochasticWeightAveraging(swa_lrs=0.0001)], #\n",
    "                        # logger = wandb_logger,\n",
    "                        precision= 32 # 'bf16', 16, 32\n",
    "    )\n",
    "    model.testPlot=True\n",
    "    \n",
    "# #     result_test = trainer.test(model, test_loader, ckpt_path='best')\n",
    "# #     result_AMC = trainer.test(model, AMC_loader, ckpt_path='best')\n",
    "# #     result_CPSC2020 = trainer.test(model, CPSC2020_loader,ckpt_path='best')\n",
    "# #     result_INCART = trainer.test(model, INCART_loader,ckpt_path='best')\n",
    "\n",
    "    result_test = trainer.test(model, test_loader)\n",
    "    result_AMC = trainer.test(model, AMC_loader)\n",
    "    result_CPSC2020 = trainer.test(model, CPSC2020_loader)\n",
    "    result_INCART = trainer.test(model, INCART_loader)\n",
    "\n",
    "\n",
    "# def test(path_ckpt, test_dataloader='test', path_root=None):\n",
    "    \n",
    "#     weight = torch.load(path_ckpt)\n",
    "#     # print(weight['hyper_parameters']['hyperparameters'])\n",
    "        \n",
    "#     classes = weight['hyper_parameters']['hyperparameters']['out_channels']\n",
    "#     test_dataset = MIT_DATASET(test_data, sr_target, classes, False)\n",
    "#     AMC_dataset = MIT_DATASET(AMC_data, sr_target, classes, False)\n",
    "#     AMCREAL_dataset = MIT_DATASET(AMCREAL_data, sr_target, classes, False)\n",
    "\n",
    "#     INCART_dataset = MIT_DATASET(INCART_data, sr_target, classes, False)\n",
    "#     Fantasia_dataset = MIT_DATASET(Fantasia_data, sr_target, classes, False)\n",
    "#     CPSC2020_dataset = MIT_DATASET(CPSC2020_data, sr_target, classes, False)\n",
    "\n",
    "#     test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = False)\n",
    "#     AMC_loader = DataLoader(AMC_dataset, batch_size = 16, shuffle = False)\n",
    "#     AMCREAL_loader = DataLoader(AMCREAL_dataset, batch_size = 16, shuffle = False)\n",
    "#     INCART_loader = DataLoader(INCART_dataset, batch_size = 16, shuffle = False)\n",
    "#     CPSC2020_loader = DataLoader(CPSC2020_dataset,batch_size = 16, shuffle = False)\n",
    "    \n",
    "#     if test_dataloader =='testMIT':\n",
    "#         dataloader = test_loader\n",
    "#     elif test_dataloader =='testAMC':\n",
    "#         dataloader = AMC_loader\n",
    "#     elif test_dataloader =='testAMCREAL':\n",
    "#         dataloader = AMCREAL_loader\n",
    "#     elif test_dataloader =='testINCART':\n",
    "#         dataloader = INCART_loader\n",
    "#     elif test_dataloader =='testFantasia':\n",
    "#         dataloader = Fantasia_loader\n",
    "#     elif test_dataloader =='testCPSC2020':\n",
    "#         dataloader = CPSC2020_loader\n",
    "        \n",
    "#     net = PVCDetection(weight['hyper_parameters']['hyperparameters'])\n",
    "#     net.load_state_dict(weight['state_dict'], strict=True)\n",
    "#     net.testPlot=True\n",
    "#     trainer = pl.Trainer(\n",
    "#         accelerator='gpu',\n",
    "#         devices=1,\n",
    "#         strategy ='dp',\n",
    "#         max_epochs=1,\n",
    "#         check_val_every_n_epoch=1,\n",
    "#         precision=32\n",
    "#     )\n",
    "    \n",
    "#     return trainer.test(net, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f5183ba-9674-4880-9490-e3387eb6cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo rm -r 20221110_final/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36fb357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0353e22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: z1it9q0m\n",
      "Sweep URL: https://wandb.ai/keewonshin/PVC_NET/sweeps/z1it9q0m\n",
      "2022-11-10 08:20:36,830 - Starting sweep agent: entity=None, project=None, count=None\n",
      "2022-11-10 08:20:37,860 - Global seed set to 42\n",
      "saving path : 20221110_final/dataSeed2_dropout0.1_featureLength512_lossFnbceloss_modelNameefficientnet-b0_norminstance_outChannels2_samplerTrue_sese_skipASPPNONE_skipModuleNONE_srTarget125_supervisionTYPE2_trainaugNEUROKIT_upsamplepixelshuffle_inChannels1/\n",
      "[16, 24, 40, 112, 320]\n",
      "skipModule:NONE\n",
      "skipASPP:ASPP NONE\n",
      "seed: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f345076c5b4025a5157e63f3ab77c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling:   0%|          | 0/3077 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-10 08:21:10,870 - GPU available: True (cuda), used: True\n",
      "2022-11-10 08:21:10,872 - TPU available: False, using: 0 TPU cores\n",
      "2022-11-10 08:21:10,873 - IPU available: False, using: 0 IPUs\n",
      "2022-11-10 08:21:10,874 - HPU available: False, using: 0 HPUs\n",
      "2022-11-10 08:21:13,543 - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "2022-11-10 08:21:13,554 - \n",
      "  | Name   | Type    | Params\n",
      "-----------------------------------\n",
      "0 | net    | UNet    | 5.6 M \n",
      "1 | lossFn | BCELoss | 0     \n",
      "-----------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "22.470    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b9f42438534706a93edd9e39f2c80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e5fa7cb7344477b53013c708de407f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "sweep_id = wandb.sweep(sweep_config, project=config_defaults['project'])\n",
    "\n",
    "wandb.agent(sweep_id,function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fdf35-fcc6-49b6-b1a3-06882f6848b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '20221110_final/dataSeed2_srTarget360_featureLength2048_samplerFalse_outChannels4_modelNameefficientnet-b0_norminstance_upsampledeconv_supervisionNONE_skipModuleNONE_trainaugNONE/weight/best_val_loss.ckpt'\n",
    "# model = PVC_NET.load_from_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e5292-6588-48cf-8487-842e5fbcb8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test('20221110_final/dataSeed2_srTarget360_featureLength2048_samplerFalse_outChannels4_modelNameefficientnet-b0_norminstance_upsampledeconv_supervisionNONE_skipModuleNONE_trainaugNONE/weight/best_val_loss.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c621d8-33cf-4ea6-9ab6-c9d29a5f68e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '20221110_final/dataSeed2_srTarget360_featureLength2048_samplerFalse_outChannels4_modelNameefficientnet-b0_norminstance_upsampledeconv_supervisionNONE_skipModuleNONE_trainaugNONE//weight/best_val_loss.ckpt'\n",
    "\n",
    "# # weight = torch.load(path)\n",
    "# # hyperparameters = weight['hyper_parameters']['hyperparameters']\n",
    "# model = PVC_NET.load_from_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23397c-2116-46f2-bbf7-95616c92ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters = config_defaults\n",
    "# PVC_NET(hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
